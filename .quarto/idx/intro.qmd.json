{"title":"Introduction","markdown":{"headingText":"Introduction","containsRefs":false,"markdown":"\nLong Short-Term Memory (LSTM) neural networks have become extremely popular in the deep learning and data science space in recent years. After being largely ignored for decades, this type of recurrent neural networks (RNN) have garnered significant attention due to their ability to model sequential data and capture long-term dependencies in data, making them perfectly suited for tasks such as language and speech processing, time series analysis, and much more. LSTM models get their power from their capacity to effectively handle the vanishing and exploding gradient problem seen with other RNN\\'s, enabling the modeling of sequential data with a great level of precision. As a result, LSTM networks have become a cornerstone of machine learning.\n\nThis paper dives into the history of LSTM neural networks, diving into previous literature and research studies conducted using LSTM. We then plan on putting an LSTM model to the test by seeing how accurately it can predict financial time series data, specifically looking at numerous companies listed on the New York Stock Exchange. While modeling financial data is an extremely difficult task due to the unpredictability of financial markets, we will explore whether they hold any potential to accurately predict short and medium term movements in stock prices. Our hope is this paper can spur further research into the ability to accurately model financial data and how such modeling can be improved in the years to come.\n\n**1.1 What is LSTM**\n\nLong short-term memory networks are a type of recurrent neural network that is specifically designed to solve sequence prediction problems. Sequence prediction problems are problems involving predicting the next value based on a given input. A simple example is the input \\[1, 2, 3, 4, 5\\] and the sequence prediction model would output \\[6\\] as the next digit in the sequence. Historically, other RNNs (Recurrent Neural Network) have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either so small that it won\\'t affect the output (vanishing gradients) or too large that it results in an overflow (exploding gradients). LSTMs overcome this challenge by regulating the weights with three \\\"gates.\\\" The Forget gate decides what information to discard from the cell, the Input gate decides which values from the input to update the memory cell, and the output gate decides what to output based on input and the memory of the cell.\n\n**1.2 Applications of LSTM Networks**\n\nLong short term memory (LSTM) models are used in a wide range of situations. The influence of the LSTM network has been notable in natural language modeling, speech recognition, machine translation, and other applications\\[1\\]. LSTM networks were mainly created to solve the exploding/vanishing gradient problem\\[1\\]. Their capacity to model and understand long range dependencies makes them critical in executing various tasks. The advantage of using LSTMs over other recurrent neural networks is that an LSTM is able to save the data for much longer periods\\[2\\] than their RNN counterparts. They are especially essential in the field of finance because they can solve several problems such as identifying complex patterns in past pricing data, forecasting stock prices and the movement of the financial markets as a whole. With their ability to handle complicated sequential data, LSTMs have revolutionized how we approach and resolve issues involving sequences of information. They have become an essential tool in many machine learning and artificial intelligence fields. \n\n**References**\n\n1.  A. Sherstinsky, \\\"Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network,\\\" Physica D: Nonlinear Phenomena, vol. 404, p. 132306, Mar. 2020, doi:[ https://doi.org/10.1016/j.physd.2019.132306](https://doi.org/10.1016/j.physd.2019.132306). \\\n    \\\n\n2.  P. Chaajer, M. Shah, and A. Kshirsagar, \\\"The applications of artificial neural networks, support vector machines, and long-short term memory for stock market prediction,\\\" Decision Analytics Journal, p. 100015, Nov. 2021, doi:[ \\\n    https://doi.org/10.1016/j.dajour.2021.100015](https://doi.org/10.1016/j.dajour.2021.100015). \\\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"intro.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","bibliography":["references.bib"],"editor":"visual","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"intro.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt"},"extensions":{"book":{}}}}}
[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Long Short-Term Memory (LSTM) neural networks have become extremely popular in the deep learning and data science space in recent years. After being largely ignored for decades, this type of recurrent neural networks (RNN) have garnered significant attention due to their ability to model sequential data and capture long-term dependencies in data, making them perfectly suited for tasks such as language and speech processing, time series analysis, and much more. LSTM models get their power from their capacity to effectively handle the vanishing and exploding gradient problem seen with other RNN’s, enabling the modeling of sequential data with a great level of precision. As a result, LSTM networks have become a cornerstone of machine learning.\nThis paper dives into the history of LSTM neural networks, diving into previous literature and research studies conducted using LSTM. We then plan on putting an LSTM model to the test by seeing how accurately it can predict financial time series data, specifically looking at numerous companies listed on the New York Stock Exchange. While modeling financial data is an extremely difficult task due to the unpredictability of financial markets, we will explore whether they hold any potential to accurately predict short and medium term movements in stock prices. Our hope is this paper can spur further research into the ability to accurately model financial data and how such modeling can be improved in the years to come.\n1.1 What is LSTM\nLong short-term memory networks are a type of recurrent neural network that is specifically designed to solve sequence prediction problems. Sequence prediction problems are problems involving predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. Historically, other RNNs (Recurrent Neural Network) have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either so small that it won’t affect the output (vanishing gradients) or too large that it results in an overflow (exploding gradients). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget gate decides what information to discard from the cell, the Input gate decides which values from the input to update the memory cell, and the output gate decides what to output based on input and the memory of the cell.\n1.2 Applications of LSTM Networks\nLong short term memory (LSTM) models are used in a wide range of situations. The influence of the LSTM network has been notable in natural language modeling, speech recognition, machine translation, and other applications[1]. LSTM networks were mainly created to solve the exploding/vanishing gradient problem[1]. Their capacity to model and understand long range dependencies makes them critical in executing various tasks. The advantage of using LSTMs over other recurrent neural networks is that an LSTM is able to save the data for much longer periods[2] than their RNN counterparts. They are especially essential in the field of finance because they can solve several problems such as identifying complex patterns in past pricing data, forecasting stock prices and the movement of the financial markets as a whole. With their ability to handle complicated sequential data, LSTMs have revolutionized how we approach and resolve issues involving sequences of information. They have become an essential tool in many machine learning and artificial intelligence fields. \nReferences\n\nA. Sherstinsky, “Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network,” Physica D: Nonlinear Phenomena, vol. 404, p. 132306, Mar. 2020, doi:https://doi.org/10.1016/j.physd.2019.132306.\n\n\nP. Chaajer, M. Shah, and A. Kshirsagar, “The applications of artificial neural networks, support vector machines, and long-short term memory for stock market prediction,” Decision Analytics Journal, p. 100015, Nov. 2021, doi:\nhttps://doi.org/10.1016/j.dajour.2021.100015."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "A. Sherstinsky, “Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network,” Physica D: Nonlinear Phenomena, vol. 404, p. 132306, Mar. 2020, doi:https://doi.org/10.1016/j.physd.2019.132306.\nP. Chaajer, M. Shah, and A. Kshirsagar, “The applications of artificial neural networks, support vector machines, and long-short term memory for stock market prediction,” Decision Analytics Journal, p. 100015, Nov. 2021, doi: https://doi.org/10.1016/j.dajour.2021.100015.\nM. Kumbure, C. Lohrmann, P. Luukka, and J. Porras, “Machine learning techniques and data for stock market forecasting: A literature review,” Expert Systems with Applications, vol. 197, p. 116659, Jul. 2022, doi: https://doi.org/10.1016/j.eswa.2022.116659\nQiao, R., Chen, W., & Qiao, Y. (2022). “Prediction of stock return by LSTM neural network,” Applied Artificial Intelligence, 36(1). doi: https://doi.org/10.1080/08839514.2022.2151159.\nFjellström, C. (2022, January 20). “Long short-term memory neural network for Financial Time Series,” arXiv.org. doi: https://doi.org/10.48550/arXiv.2201.08218.\nMehlig, B. (2021, October 27). “Machine learning with neural networks,” arXiv.org. doi: https://doi.org/10.48550/arXiv.1901.05639.\nStaudemeyer, R., Morris, E. (2019, September 23). “Understanding LSTM – a tutorial into Long Short-Term Memory Recurrent Neural Networks,” ArXiv.org. doi: https://doi.org/10.48550/arXiv.1909.09586\nSharma, S., Athaiya, A. (2020, April). “ACTIVATION FUNCTIONS IN NEURAL NETWORKS,” International Journal of Engineering Applied Sciences and Technology. 04. 310-316. 10.33564/IJEAST.2020.v04i12.054."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LSTM Recurrent Neural Networks",
    "section": "",
    "text": "Preface\nOur analysis and report on Long Short-Term Memory (LSTM) recurrent neural networks."
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "2  Data",
    "section": "",
    "text": "This project will be focused on predicting the percent price change on a given day from historical financial data markets. For this analysis, we’ll be using historical time-series data for Apple, a corporation listed on the New York Stock Exchange. That company is Apple, and we chose Apple due to its high trading volume and relatively low volatility. At the end of our analysis, we’ll also compare the results to a second company also traded on the NYSE, General Motors.\nWe decided to pull data going back the last 5 years, although this was an arbitrary decision. The data is sourced from Yahoo Finance using the yfinance package in python. yfinance is an open-source package that uses Yahoo Finance’s publicly available APIs to download market data.\n2.1 Data Variables Description\nSeveral variables are retrieved for each stock. The definitions for the variables are as follows :\nDate : This represents the date associated with each data point, such as daily stock prices.\nOpen : The stock price at the time of opening on a given trading day\nClose : The stock price at the time of closing on a given trading day\nHigh : The highest stock price reached on a given day.\nLow : The lowest stock price reached on a given day.\nDividends : The amount of dividends paid to shareholders.\nSplit ratio : If there have been stock splits, this variable indicates the split ratio.\nAdj Close : The closing stock price adjusted for any corporate actions such as dividends and stock splits.\nVolume : The total number of shares traded on a given day.\nMarket Capitalization : The total market value of the market’s shares.\nIn addition to the variables to the above variables, we are also going to add two more variables.\nPCT Change - this calculates the percentage change of the closing price.\nMoving Average - this variable will represent the average change of closing price for the series.\n2.2 Data Visualizations"
  },
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "3  Methodology",
    "section": "",
    "text": "This is the methodology section\n3.1\n3.2\n3.3 Implementation\nOur LSTM model for this project will be implemented in Python using the scikit-learn machine learning library and the Keras package. Keras is a high-level, deep learning API developed by Google for implementing neural networks built on top of the TensowFlow platform. Scikit-learn will be used to prepare our numeric dataset for an LSTM network, and to split our data into an 80/20 training set and testing set. The data must be scaled to achieve the best results. Large inputs tend to slow down the learning convergence. We have two options for scaling our data, normalization, and standardization. Normalization takes our data and outputs it in range between [0-1]. Standardization re-scales our data so that the mean is 0 and the standard deviation equals 1. For this project we will implement normalization. We will accomplish this by importing MinMaxScaler from the sklearn.preprocessing library. We will then create a MinMaxScaler object defining a range between 0-1, and lastly run the fit_transform method on our data set. To split our data, we will create 4 variables, LSTM_Xtrain, LSTM_Xtest, LSTM_ytrain, and LSTM_ytest to hold our training/testing sets. This will be done using the train_test_split() method that is part of the Scikit-learn library."
  },
  {
    "objectID": "intro.html#what-is-lstm",
    "href": "intro.html#what-is-lstm",
    "title": "1  Introduction",
    "section": "1.1 What is LSTM?",
    "text": "1.1 What is LSTM?\nLong short-term memory networks are a type of recurrent neural network that is specifically designed to solve sequence prediction problems. Sequence prediction problems are problems involving predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. Historically, other RNNs (Recurrent Neural Network) have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either so small that it won’t affect the output (vanishing gradients) or too large that it results in an overflow (exploding gradients). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget gate decides what information to discard from the cell, the Input gate decides which values from the input to update the memory cell, and the output gate decides what to output based on input and the memory of the cell."
  },
  {
    "objectID": "intro.html#applications-of-lstm-networks",
    "href": "intro.html#applications-of-lstm-networks",
    "title": "1  Introduction",
    "section": "1.2 Applications of LSTM Networks",
    "text": "1.2 Applications of LSTM Networks\nLong short term memory (LSTM) models are used in a wide range of situations. The influence of the LSTM network has been notable in natural language modeling, speech recognition, machine translation, and other applications[1]. LSTM networks were mainly created to solve the exploding/vanishing gradient problem[1]. Their capacity to model and understand long range dependencies makes them critical in executing various tasks. The advantage of using LSTMs over other recurrent neural networks is that an LSTM is able to save the data for much longer periods[2] than their RNN counterparts. They are especially essential in the field of finance because they can solve several problems such as identifying complex patterns in past pricing data, forecasting stock prices and the movement of the financial markets as a whole. With their ability to handle complicated sequential data, LSTMs have revolutionized how we approach and resolve issues involving sequences of information. They have become an essential tool in many machine learning and artificial intelligence fields."
  },
  {
    "objectID": "Data.html#data-description",
    "href": "Data.html#data-description",
    "title": "2  Data",
    "section": "2.1 Data Description",
    "text": "2.1 Data Description\nThe resulting data file is relatively simple, comprised of only seven columns, with each row representing an individual trading day going back to five years. The NYSE is only open on weekdays, excluding most Federal holidays. Thus, our data set contains 1,258 trading days, the amount of days in the last five years the stock market was at least partially open."
  },
  {
    "objectID": "Data.html#standard-variables",
    "href": "Data.html#standard-variables",
    "title": "2  Data",
    "section": "2.1 Standard Variables",
    "text": "2.1 Standard Variables\nDate : This represents a specific trading day that the market was at least partially open.\nOpen : The price at the time of market opening on the given day.\nHigh : The highest price that the stock reached on the given day.\nLow : The lowest price that the stock reached on the given day.\nClose : The price at the time of market closing on the given day.\nAdj Close : The closing stock price adjusted for any corporate actions such as dividends and stock splits.\nVolume : The total number of shares traded on the given day.\n\n\n\n\n\nTable 1: Preview of the first 5 rows of the dataset"
  },
  {
    "objectID": "Data.html#additional-variables",
    "href": "Data.html#additional-variables",
    "title": "2  Data",
    "section": "2.2 Additional Variables",
    "text": "2.2 Additional Variables\nIn addition to the variables to the standard variables fetched from the Yahoo Finance API, we created four of our own columns to add to the dataset.\nCompany Name : This variable simply denotes the company name that the data set belongs to.\nPercent Change : This variable calculates what percent the price changed from the previous closing price.\nIt’s important because we are trying to predict what percent the price will shift.\nThe percent change variable was created using the pandas “pct_change” command, applying it to the closing price only going back one period, thus creating a variable showing what percent the price moved each trading day, compared to the previous trading day.\nAAPL['pct_change'] = AAPL.Close.pct_change(periods = 1)\nSimple Moving Average : A simple moving average (SMA) is created by taking the mean closing price of a stock over a given number of periods. We created the SMA going back 20 time periods, so that each value represents the average closing price of the last 20 trading days.\nThis variable was created by using the pandas “rolling” command going back 20 periods. Because the first 19 days of the dataset (the oldest ones) do not have 20 previous periods to look to, they will instead have NULL values. We’ll drop those 19 rows so that our dataset contains no null values.\nAAPL['SMA20'] = AAPL['Close'].rolling(20).mean()\nExponential Moving Average : Given that we are trying to predict the price change of a stock in the short-term, exponential weighted moving averages (EMA) provide a benefit that SMA’s do not. We can generally assume that tomorrow’s stock price for a given company is going to be influenced by today’s closing price as compared to the closing price 20 days ago, SMA’s contain an inherent weakness for our analysis. SMA’s weigh each price point the same, so that in a 20 series average, each price point is weighed at an even 5%. EMA’s give additional weight to the most recent days, so day 20 will have a higher weight than day 19, which will have a higher weight than day 18, and so on.\nThis variable was created by using the pandas “ewm” command going back 20 periods.\nAAPL['EMA20'] = AAPL['Close'].ewm(span=20).mean()"
  },
  {
    "objectID": "Data.html#data-visualizations",
    "href": "Data.html#data-visualizations",
    "title": "2  Data",
    "section": "2.3 Data Visualizations",
    "text": "2.3 Data Visualizations"
  },
  {
    "objectID": "Data.html#data-summary-visualizations",
    "href": "Data.html#data-summary-visualizations",
    "title": "2  Data",
    "section": "2.3 Data Summary & Visualizations",
    "text": "2.3 Data Summary & Visualizations\nWe know that outside of the date variable, which acts as the index for this dataset, there are 6 other data points that came from the API. We created four more variables and dropped null values. Our resulting dataset contains 1,238 trading days. We can tell from using the pandas “info” command that all the variables are floats, with the exception of volume, which is an integer, and company name, which is an object. We also verify that there are no null values in our data.\n\n\n\n\n\nFigure 1: Dataframe information of the dataset.\nBy running the “describe” function, we can get a better idea of our dataset.\n\n\n\n\n\nTable 2: Summary statistics of the dataset\nDuring our five-year period, Apple’s closing price has ranged from $34.16 to $196.19, with the mean closing price being $117.78 and the median closing price at $130.86.\nWhen we plot the closing price by date, we can see that the price has been moving steadily upwards throughout the five year period, with occasional decreases.\n\n\n\n\n\nFigure 2: Closing price by day for AAPL\nFrom the “describe” function pictured above, we can also tell that the percent change of the stock price has ranged as low as 12.8% lower from the previous trading days closing price, to as high as 11.9% higher. As we are trying to predict percent change, it was important to see a histogram of the datapoint.\n\n\n\n\n\nFigure 3: Histogram of the daily percent change for AAPL\nThe histogram shows that the data is pretty evenly distributed, with a slight right tail."
  },
  {
    "objectID": "intro.html#what-is-an-lstm-network",
    "href": "intro.html#what-is-an-lstm-network",
    "title": "1  Introduction",
    "section": "1.1 What is an LSTM Network?",
    "text": "1.1 What is an LSTM Network?\nLSTM networks are a type of recurrent neural network that works well with sequential data, such as time series data and text. LSTMs are designed to solve sequence prediction problems. Such sequence prediction problems involve predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. In theory, RNN’s would be able to solve such a problem. Historically, however, and when dealing with real-world datasets, RNN’s have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either too small that it won’t affect the output (vanishing gradients problem) or too large that it results in an overflow (exploding gradients problem). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget Gate decides what information to discard from the cell, the Input Gate decides which values from the input to update the memory cell, and the Output Gate decides what to output based on input and the memory of the cell."
  },
  {
    "objectID": "intro.html#previous-research",
    "href": "intro.html#previous-research",
    "title": "1  Introduction",
    "section": "1.3 Previous Research",
    "text": "1.3 Previous Research\nTrying to predict stock market returns is a tale as old as time. There has been extensive research published on the ability to forecast financial markets through machine learning [3]. However, very few papers have focused specifically on LSTM models, which are ideal for such a time-series prediction. These papers have all been published in the last few years, making research on this topic novel and additional research vital to further understand the prediction value of LSTM neural networks as it applies to the financial markets.\nIn one 2022 paper conducted by researchers at a Chinese university, the authors applied an LSTM model to Chinese stock market data going back three years and found that the LSTM model performed better than linear regression model [4]. They concluded their research by stating that “stock returns are predictable to some extent” and that LSTM models “can help improve the prediction” potential of such returns.\nAnother 2022 paper, this one published Swedish researcher Dr. Carmina Fjellström, looked at the ability of LSTM models to select better-performing stock portfolios [5]. She took a Swedish stock index with the 29 most-traded companies in Sweden (OMX30) and downloaded the daily closing prices dating back 18 years. After running an LSTM model, she compared the results of the portfolio selected by the LSTM to the index as a whole (all 29 stocks), as well as to a randomly chosen portfolio. Dr. Fjellström found that LSTM portfolio had the highest average daily return of the three."
  },
  {
    "objectID": "methodology.html#neural-networks",
    "href": "methodology.html#neural-networks",
    "title": "3  Methodology",
    "section": "3.1 Neural Networks",
    "text": "3.1 Neural Networks\nGiven that LSTM models are an advanced form of RNN, one must first understand how RNN’s work and their limitations, so one can get an understanding of why LSTM models are needed. Before getting there, it’s worth a quick introduction into Neural Networks themselves.\nNeural Networks (also known as Artificial Neural Networks or ANN’s) are based on the same principle on how the human brain works: a network of neuron’s [6]. The network improves and learns by changing the connections between each neuron, which can also be called a node. In the case of machine learning, a neural network can take a supervised set of data with class labels and create nodes based on various input variables, and optimize the connections between each node using optimal strengths to be able to accurately predict unseen data. Thus, the network is made up entirely of individual nodes and individual connections between the nodes.\nA neural network must have at least one input node, at least one output node, and at least one hidden layer, which are layers of nodes between the Input and Output nodes. Often times, it will contain multiple inputs, hidden layers and outputs (Figure 4)\n\n\n\n\n\nFigure 4: An illustration of a neural network with two input nodes in the input layer, two hidden layers with four nodes each, and two output nodes, with a spider-web of connections between them.\nThere are various learning techniques that one can set up for a neural network, called learning rules. The most common learning rule is backpropagation [7]. It uses gradient descent to learn the weights of each node, which are essentially the connections between the nodes. The goal of gradient descent is to take small, incremental steps meant at minimizing a loss function, such as the Mean-Squared Error (MSE), for each weight. The activation function (sometimes called the transfer function) takes those weights and defines an output based on the activation function formula. That output will then determine whether any specific neuron is activated. Neurons that are activated have their outputs fed to the next step of the training process.\nThere are various activation functions and their usefulness depends on various factors, including whether the data is linear or non-linear. There are three activation functions that are the most popular: Sigmoid, Tanh and ReLU. The Sigmoid function maps inputs, regardless of size, to an output ranging from 0 to 1. The Tanh function returns an output ranging from -1 to 1, while the ReLU function removes any negative part of the resulting outputs [8] (Figure 5).\n\n\n\n\n\nFigure 5: An illustration of the three most popular activation functions.\n\n3.1.1 Recurrent Neural Networks\nMost ANN’s are feed-forward neural networks, so they are all fully connected and loop free. This means that each neuron in the network provides an input to a neuron in the following layer, never going back and sending an input to a neuron in a previous layer [8]. While this can be useful for some tasks, one of the most powerful benefits that ANN’s have is their ability to build a memory of time-series events by going back in time. This is done by having a circular connection between each layer, thus creating an Recurrent Neural Network. Unlike ANN’s, a RNN uses that circular connection to previous steps, therefore going back in time (Figure 6).\n\n\n\n\n\nFigure 6: An illustration of a recurrent neural network. Unlike an Artificial Neural Network, an RNN contains a feedback loop between the hidden layers.\nRNN’s also need to be trained differently, as backpropgation on its own does not feed any information back to previous steps. In order to properly train an RNN, we need a feedback loop to do just that. There are two methods that are most common when training an RNN: Backpropagation through time (BPTT) and Real-time recurrent learning (RTRL), the only difference between the two being how they optimize the weights [8].\nDuring that training of an RNN, regardless of which method is used, we run into the issue of exploding and vanishing gradients. During either BPTT or RTRL, a given weight is multiplied by each input in the network. This essentially means the weight is multiplied to the power of how many inputs there are. Take an example of a stock market prediction neural network where 100 days of previous trading data act as the inputs. If a given weight is greater than 1, let’s say in this example the weight is 1.5, one would multiply 1.5 to the power of 100, resulting in an number over 400 quadrillion. Because the goal of the learning rule is to take small, incremental steps to find the optimal weights, this extremely large number will cause the gradient descent to take extremely large steps instead of finding that optimal weight, resulting in an exploding gradient [8].\nThis problem presents itself any time the weight is greater than 1. If you set the weight to below 1, you run into the opposite problem. Using the same stock market prediction, an example weight of 0.5 would then be multiplied to the 100th power, resulting in a number so small that it’s essentially zero. This causes the gradient descent to take steps that too small, never allowing it to find the best parameter values for the network, resulting in a vanishing gradient [8]."
  },
  {
    "objectID": "methodology.html#section-3.2",
    "href": "methodology.html#section-3.2",
    "title": "3  Methodology",
    "section": "3.2 Section 3.2",
    "text": "3.2 Section 3.2"
  },
  {
    "objectID": "methodology.html#implementation",
    "href": "methodology.html#implementation",
    "title": "3  Methodology",
    "section": "3.3 Implementation",
    "text": "3.3 Implementation\nOur LSTM model for this project will be implemented in Python using the scikit-learn machine learning library and the Keras package. Keras is a high-level, deep learning API developed by Google for implementing neural networks built on top of the TensowFlow platform. Scikit-learn will be used to prepare our numeric dataset for an LSTM network, and to split our data into an 80/20 training set and testing set. The data must be scaled to achieve the best results. Large inputs tend to slow down the learning convergence. We have two options for scaling our data, normalization, and standardization. Normalization takes our data and outputs it in range between [0-1]. Standardization re-scales our data so that the mean is 0 and the standard deviation equals 1. For this project we will implement normalization. We will accomplish this by importing MinMaxScaler from the sklearn.preprocessing library. We will then create a MinMaxScaler object defining a range between 0-1, and lastly run the fit_transform method on our data set. To split our data, we will create 4 variables, LSTM_Xtrain, LSTM_Xtest, LSTM_ytrain, and LSTM_ytest to hold our training/testing sets. This will be done using the train_test_split() method that is part of the Scikit-learn library."
  },
  {
    "objectID": "methodology.html#long-short-term-memory",
    "href": "methodology.html#long-short-term-memory",
    "title": "3  Methodology",
    "section": "3.2 Long Short-Term Memory",
    "text": "3.2 Long Short-Term Memory\nLSTMs are an improvement on the standard RNN models desgined to deal with more complex problems. Unlike a traditional RNN (which also involve a loop),they are able to handle long-term dependencies.. LSTMs were specifically designed to overcome the issues that come with training other RNNs such as vanishing and exploding gradient issues. In order to make the required prediction, they need to process not just individual data points but the entire sequence of data.They are composed of four components; the cell,input gate, the output gate and a forget gate [1].\n\n\n\nAN ILLUSTRATION OF LSTM NETWORK\n\n\nThe gates are responsible for processing and regulating the information flowing in and out of the cell. The cell state carries the relevant information during the process of the sequence. Information is continually added to and removed from cell through the gates. The gates determine which information is important to keep or forget during the process.\nThe forget gate is responsible for discarding the information which is not gonna used in the next steps. It determines which information from the previous cell state is retained[9]. The closer the value of the information passed through the forget is to 0, the more likely is to be forgotten.Therefore this means that the closer the value is to 1, the more likely it is to be kept. The input gate charge of choosing which new data points to include in the cell state at a certain time step [9]. The input gate regulates the information flow by producing values between 0 and 1 using a sigmoid activation function. The output gate decides which cell state data should be carried over to the following time step and added to the network’s output[9]. It makes use of a sigmoid activation function and is essential to the LSTM’s capacity to provide outputs that are aware of context.\nThe sigmoid and tanh activation functions are crucial for the different gates and operations of the LSTM architecture in a typical LSTM network. Within the network, these activation functions are used to manage information flow and determine what should be updated, remembered, and forgotten.\n\n\n\nAN ILLUSTRATION OF ACTIVATION FUNCTIONS USED BY LSTM NETWORKS\n\n\nThe sigmoid function is employed in the input, output and forget gates. In the input gate it is used to calculate the values from the current input and the previous hidden state to be between 0 and 1. In the forget gate, it calculates the input from the previous hidden state and current input . In the output gate, the sigmoid function also calculates the values of the current input and previous hidden state to produce the output . The tanh function is specifically employed in the output gate to determine which information from the cell state is going to be included in the hidden state and eventually in the network's output ."
  },
  {
    "objectID": "Analysis.html",
    "href": "Analysis.html",
    "title": "4  Analysis",
    "section": "",
    "text": "#Downloading the Data\n\n#!pip install tensorflow\n#!pip install keras\n#!pip install matplotlib\n\nimport yfinance as yf\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib\n\n#Stocks that we'll be analyzing\nAAPL = ['AAPL']\nGM = ['GM']\n\n#Set start and end date for the data pull\n#We want to look at the past 5 years, so we'll pull the previous 5 years of data\nend_time = datetime.now()\nstart_time = datetime(end_time.year - 5, end_time.month, end_time.day)\n\n#download the stocks we want to model \nAAPL = yf.download(AAPL, start_time, end_time)\nGM = yf.download(GM, start_time, end_time)\n\n\n#Adding additional columns\nAAPL['pct_change'] = AAPL.Close.pct_change(periods = 1)\nAAPL['EMA20'] = AAPL['Close'].ewm(span=20).mean()\n\nGM['pct_change'] = GM.Close.pct_change(periods = 1)\nGM['EMA20'] = GM['Close'].ewm(span=20).mean()\n\n#AAPL.head(10)\n\n#Correlation Analysis \n#correlation = AAPL.corr()\n#print(correlation['Close'].sort_values(ascending=False))\n\n#Defining X and y variable\nX1 = AAPL[['Open','High', 'Low', 'Volume']]\nX1.head(10)\ny1 = AAPL['Close']\ny1.head(10)\n\nX2 = GM[['Open','High', 'Low', 'Volume']]\nX2.head(10)\ny2 = GM['Close']\ny2.head(10)\n\n#Converting to array\nX1 = X1.to_numpy()\ny1 = y1.to_numpy()\n\nX2 = X2.to_numpy()\ny2 = y2.to_numpy()\n\n#Splitting our data into 80/20 training/testing sets\nfrom sklearn.model_selection import train_test_split\nLSTM_Xtrain1, LSTM_Xtest1, LSTM_ytrain1, LSTM_ytest1 = train_test_split(X1, y1, test_size=0.2, random_state=1)\n\nLSTM_Xtrain2, LSTM_Xtest2, LSTM_ytrain2, LSTM_ytest2 = train_test_split(X2, y2, test_size=0.2, random_state=1)\n\nprint(LSTM_Xtrain1)\nprint(LSTM_Xtrain2)\n\n# Stacked LSTM model \n\nimport tensorflow as tf\nimport keras as ke\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\n#Apple model\nLSTM_modelApple = ke.Sequential()\nLSTM_modelApple.add(LSTM(128, return_sequences=True, input_shape=(LSTM_Xtrain1.shape[1], 1)))\nLSTM_modelApple.add(LSTM(64, return_sequences=False))\nLSTM_modelApple.add(Dense(25, activation='linear'))\nLSTM_modelApple.add(Dense(1))\n\n#GM model\nLSTM_modelGM = ke.Sequential()\nLSTM_modelGM.add(LSTM(128, return_sequences=True, input_shape=(LSTM_Xtrain2.shape[1], 1)))\nLSTM_modelGM.add(LSTM(64, return_sequences=False))\nLSTM_modelGM.add(Dense(25, activation='linear'))\nLSTM_modelGM.add(Dense(1))\n\nLSTM_modelApple.summary()\nLSTM_modelGM.summary()\n\nLSTM_modelApple.compile(optimizer='rmsprop' , loss= 'mean_squared_error')\n\nLSTM_modelGM.compile(optimizer='rmsprop' , loss= 'mean_squared_error')\n\nhistory1 = LSTM_modelApple.fit(LSTM_Xtrain1, LSTM_ytrain1, batch_size=32, epochs=10)\n\nhistory2 = LSTM_modelGM.fit(LSTM_Xtrain2, LSTM_ytrain2, batch_size=32,epochs=10)\n\n\nloss_test1= LSTM_modelApple.evaluate(LSTM_Xtest1, LSTM_ytest1)\nloss_test2 = LSTM_modelApple.evaluate(LSTM_Xtest2, LSTM_ytest2)\n\npredictions1 = LSTM_modelApple.predict(LSTM_Xtest1)\n\npredictions1.reshape(252,)\n\npredictions2 = LSTM_modelGM.predict(LSTM_Xtest2)\n\npredictions2.reshape(252,)\n\n#Predict 11/03/23 price = $177.57\nIn_featuresApple = np.array([[175.52, 177.78, 175.46, 76083900]])\nLSTM_modelApple.predict(In_featuresApple)\nprint(\"The predicted stock price for 11/03/23 is $ \", LSTM_modelApple.predict(In_featuresApple),\".\", \" The actual stock price for 11/03/23 is $177.57\")\n\n#Plot the first 50 predictions vs the actual y values\nimport matplotlib.pyplot as plt\nLSTM_ytest1 = LSTM_ytest1.reshape(252, 1)\n\nLSTM_ytest2 = LSTM_ytest2.reshape(252, 1)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n[[1.72259995e+02 1.73630005e+02 1.70820007e+02 4.95946000e+07]\n [1.61529999e+02 1.62470001e+02 1.61270004e+02 4.95017000e+07]\n [1.14570000e+02 1.15230003e+02 1.10000000e+02 1.80860300e+08]\n ...\n [1.57970001e+02 1.58490005e+02 1.55979996e+02 4.59922000e+07]\n [6.11274986e+01 6.12000008e+01 6.04524994e+01 6.92752000e+07]\n [1.52570007e+02 1.53100006e+02 1.50779999e+02 6.98583000e+07]]\n[[3.20499992e+01 3.21899986e+01 3.13099995e+01 1.30609000e+07]\n [3.62500000e+01 3.64799995e+01 3.58699989e+01 9.30020000e+06]\n [3.02999992e+01 3.05699997e+01 2.98700008e+01 1.36387000e+07]\n ...\n [3.44599991e+01 3.46500015e+01 3.40499992e+01 1.22142000e+07]\n [3.66199989e+01 3.67700005e+01 3.57400017e+01 6.71880000e+06]\n [4.09000015e+01 4.16100006e+01 4.07700005e+01 1.42857000e+07]]\n\n\nModel: \"sequential\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm (LSTM)                 (None, 4, 128)            66560     \n\n\n                                                                 \n\n\n lstm_1 (LSTM)               (None, 64)                49408     \n\n\n                                                                 \n\n\n dense (Dense)               (None, 25)                1625      \n\n\n                                                                 \n\n\n dense_1 (Dense)             (None, 1)                 26        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 117619 (459.45 KB)\n\n\nTrainable params: 117619 (459.45 KB)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\nModel: \"sequential_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_2 (LSTM)               (None, 4, 128)            66560     \n\n\n                                                                 \n\n\n lstm_3 (LSTM)               (None, 64)                49408     \n\n\n                                                                 \n\n\n dense_2 (Dense)             (None, 25)                1625      \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 26        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 117619 (459.45 KB)\n\n\nTrainable params: 117619 (459.45 KB)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\nEpoch 1/10\n\n\n 1/32 [..............................] - ETA: 5:14 - loss: 17850.5234\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 15644.5586  \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 15900.7969\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 15772.7217\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 14881.7344\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 14459.6035\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 14024.1689\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 13654.3691\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 13179.0322\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/32 [========================>.....] - ETA: 0s - loss: 13086.1475\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/32 [===========================>..] - ETA: 0s - loss: 12742.4707\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 11s 21ms/step - loss: 12606.0127\n\n\nEpoch 2/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 8693.1504\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 9772.4922\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 9170.6875\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 9002.3066\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 8641.8262\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 8623.2207\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 8586.4473\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 8393.2275\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 8248.5586\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 8114.9932\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 7880.6470\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 7868.6719\n\n\nEpoch 3/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 5713.8296\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 5922.1846\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 5842.7729\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 5897.3521\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 5945.6724\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 5823.5977\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 5688.9473\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 5461.6406\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 5356.8721\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/32 [========================>.....] - ETA: 0s - loss: 5271.1655\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/32 [===========================>..] - ETA: 0s - loss: 5127.4526\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 22ms/step - loss: 5041.5947\n\n\nEpoch 4/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 3034.0632\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 3556.1946\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 3487.9436\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 3493.4324\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 3464.1040\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 3481.0308\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 3361.7490\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 3292.1914\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 3271.4468\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 3220.5398\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 3095.1345\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 3089.4897\n\n\nEpoch 5/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 2102.0312\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 1699.1125\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 1639.6678\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 1590.3613\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 1472.7759\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 1409.2086\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 1361.2793\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 1301.4639\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 1238.5306\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 1204.3831\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 1164.0266\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 1155.6792\n\n\nEpoch 6/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 619.1258\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 828.0585\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 856.7490\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 716.8060\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 632.5975\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 573.7088\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 547.6782\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 518.9468\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 487.2103\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 465.4656\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 451.3146\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 448.6695\n\n\nEpoch 7/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 347.0085\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/32 [=>............................] - ETA: 0s - loss: 296.7923\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 245.1817\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 219.7252\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 223.0440\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 249.5138\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/32 [==============>...............] - ETA: 0s - loss: 227.5485\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/32 [=================>............] - ETA: 0s - loss: 217.0573\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/32 [====================>.........] - ETA: 0s - loss: 211.6015\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 210.4924\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 203.7533\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 219.6363\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 219.6363\n\n\nEpoch 8/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 213.4327\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 133.0811\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 108.0463\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 118.1999\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 159.1105\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 139.9008\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 139.5156\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 135.0309\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 128.4955\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 141.6774\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 135.5284\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 134.5963\n\n\nEpoch 9/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 55.0438\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 107.0401\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 94.6184 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 155.8024\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 129.3708\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 112.3770\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 112.7955\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 131.7285\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 126.4455\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 119.4833\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 113.7246\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 113.8472\n\n\nEpoch 10/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 127.4089\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 76.9268 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 54.0622\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 45.3098\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 103.6735\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 98.6646 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 88.8192\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 80.0932\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 81.8757\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 88.9611\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 89.0983\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 88.6498\n\n\nEpoch 1/10\n\n\n 1/32 [..............................] - ETA: 4:48 - loss: 1773.3213\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 1537.0836  \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 1352.1608\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 1211.0248\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 1084.7893\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 957.5520 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 857.1149\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 774.2867\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 708.5995\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 652.3316\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 606.9402\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 10s 21ms/step - loss: 600.7240\n\n\nEpoch 2/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 80.7045\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 121.5190\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 145.9119\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 132.5465\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 119.7521\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 115.8692\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 113.3471\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 108.6190\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 108.2770\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 108.7407\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 109.9512\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 109.6814\n\n\nEpoch 3/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 68.2729\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 78.9483\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 83.9708\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 93.3768\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 92.0741\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 93.3890\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 90.5298\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 89.4009\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 85.3424\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 80.2891\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 78.1326\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 77.3099\n\n\nEpoch 4/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 45.8507\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 23.3488\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 23.6393\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 24.5916\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 21.8899\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 19.2271\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 19.8572\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 18.3936\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 16.5420\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 15.6097\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 15.4025\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 15.2536\n\n\nEpoch 5/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 4.4074\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 5.7328\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/32 [====>.........................] - ETA: 0s - loss: 5.1036\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/32 [=======>......................] - ETA: 0s - loss: 6.1751\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 10.4035\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 9.9158 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 8.3925\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 7.2127\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 6.7097\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 8.8793\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 8.5070\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 7.8676\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 22ms/step - loss: 7.8354\n\n\nEpoch 6/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 3.7876\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 4.6281\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 5.3822\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 8.0719\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 7.0538\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 6.0123\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 5.7606\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 7.2321\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 7.8451\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 7.1664\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 6.5498\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 6.4707\n\n\nEpoch 7/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 1.7170\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 1.7070\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 9.4125\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 8.4469\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 6.7784\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 5.7227\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 6.9527\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 8.6763\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 7.7715\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 7.0259\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 6.4263\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 6.3665\n\n\nEpoch 8/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 5.3137\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 9.4058\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 8.6938\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 6.4935\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 5.7911\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 6.1708\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 7.2777\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 7.3643\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 6.6420\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 6.1092\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 5.9298\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 5.9926\n\n\nEpoch 9/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 15.8345\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 9.6827 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 8.1831\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 6.1453\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 5.2620\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 5.0287\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 6.4515\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 5.9791\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 5.5776\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 5.2321\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 4.9010\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 5.6360\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 22ms/step - loss: 5.6360\n\n\nEpoch 10/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 10.8725\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 5.5644 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 3.6194\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 2.9519\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 4.3095\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 4.6350\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 4.1252\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 4.0169\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 4.1873\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 4.4110\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 4.3664\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 4.3573\n\n\n1/8 [==>...........................] - ETA: 14s - loss: 41.7166\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s - loss: 56.9148 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 2s 10ms/step - loss: 55.5127\n\n\n1/8 [==>...........................] - ETA: 0s - loss: 10.9799\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s - loss: 13.5169\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 10ms/step - loss: 13.5051\n\n\n1/8 [==>...........................] - ETA: 16s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 2s 9ms/step\n\n\n1/8 [==>...........................] - ETA: 13s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 2s 10ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 61ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 54ms/step\n\n\nThe predicted stock price for 11/03/23 is $  [[164.19055]] .  The actual stock price for 11/03/23 is $177.57\n\n\nApple Stock Price Prediction\n\nplt.plot(LSTM_ytest1[:50], 'red', label = 'AAPL Stock Price')\nplt.plot(predictions1[:50], color = 'green', label = 'Predicted AAPL Stock Price')\nplt.title('AAPL Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('AAPL Stock Price')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\nGM Stock Price Prediction\n\nplt.plot(LSTM_ytest2[:50], 'red', label = 'GM Stock Price')\nplt.plot(predictions2[:50], color = 'green', label = 'Predicted GM Stock Price')\nplt.title('GM Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('GM Stock Price')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\nTraining loss for Apple\n\nplt.close()\nplt.plot(history1.history['loss'], label='Loss')\nplt.title('Training Loss for Apple')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\nTraining loss plot for GM\n\nplt.close()\nplt.plot(history2.history['loss'], label='Loss')\nplt.title('Training Loss for GM')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\nThe following table illustrates the fact that different numbers of epochs can yield different results when it comes to training. Training for an excessive number of epochs can cause overfitting, where the model begins to memorize the training data instead of generalizing well to new, unseen data, and underfitting, where the model fails to capture the underlying patterns in the data. Achieving the best training outcomes requires finding the correct balance.\n\n\n\nEpochs\nApple\nGM\n\n\n\n\n25\n\n\n\n\n50\n\n\n\n\n100\n\n\n\n\n150\n\n\n\n\n200"
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "Conclusion section"
  }
]
[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Long Short-Term Memory (LSTM) neural networks have become extremely popular in the deep learning and data science space in recent years. After being largely ignored for decades, this type of recurrent neural networks (RNN) have garnered significant attention due to their ability to model sequential data and capture long-term dependencies in data, making them perfectly suited for tasks such as language and speech processing, time series analysis, and much more. LSTM models get their power from their capacity to effectively handle the vanishing and exploding gradient problem seen with other RNN’s, enabling the modeling of sequential data with a great level of precision. As a result, LSTM networks have become a cornerstone of machine learning.\nThis paper dives into the history of LSTM neural networks, diving into previous literature and research studies conducted using LSTM. We then plan on putting an LSTM model to the test by seeing how accurately it can predict financial time series data, specifically looking at numerous companies listed on the New York Stock Exchange. While modeling financial data is an extremely difficult task due to the unpredictability of financial markets, we will explore whether they hold any potential to accurately predict short and medium term movements in stock prices. Our hope is this paper can spur further research into the ability to accurately model financial data and how such modeling can be improved in the years to come.\n1.1 What is LSTM\nLong short-term memory networks are a type of recurrent neural network that is specifically designed to solve sequence prediction problems. Sequence prediction problems are problems involving predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. Historically, other RNNs (Recurrent Neural Network) have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either so small that it won’t affect the output (vanishing gradients) or too large that it results in an overflow (exploding gradients). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget gate decides what information to discard from the cell, the Input gate decides which values from the input to update the memory cell, and the output gate decides what to output based on input and the memory of the cell.\n1.2 Applications of LSTM Networks\nLong short term memory (LSTM) models are used in a wide range of situations. The influence of the LSTM network has been notable in natural language modeling, speech recognition, machine translation, and other applications[1]. LSTM networks were mainly created to solve the exploding/vanishing gradient problem[1]. Their capacity to model and understand long range dependencies makes them critical in executing various tasks. The advantage of using LSTMs over other recurrent neural networks is that an LSTM is able to save the data for much longer periods[2] than their RNN counterparts. They are especially essential in the field of finance because they can solve several problems such as identifying complex patterns in past pricing data, forecasting stock prices and the movement of the financial markets as a whole. With their ability to handle complicated sequential data, LSTMs have revolutionized how we approach and resolve issues involving sequences of information. They have become an essential tool in many machine learning and artificial intelligence fields. \nReferences\n\nA. Sherstinsky, “Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network,” Physica D: Nonlinear Phenomena, vol. 404, p. 132306, Mar. 2020, doi:https://doi.org/10.1016/j.physd.2019.132306.\n\n\nP. Chaajer, M. Shah, and A. Kshirsagar, “The applications of artificial neural networks, support vector machines, and long-short term memory for stock market prediction,” Decision Analytics Journal, p. 100015, Nov. 2021, doi:\nhttps://doi.org/10.1016/j.dajour.2021.100015."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "A. Sherstinsky, “Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network,” Physica D: Nonlinear Phenomena, vol. 404, p. 132306, Mar. 2020, doi:https://doi.org/10.1016/j.physd.2019.132306.\nP. Chaajer, M. Shah, and A. Kshirsagar, “The applications of artificial neural networks, support vector machines, and long-short term memory for stock market prediction,” Decision Analytics Journal, p. 100015, Nov. 2021, doi: https://doi.org/10.1016/j.dajour.2021.100015.\nM. Kumbure, C. Lohrmann, P. Luukka, and J. Porras, “Machine learning techniques and data for stock market forecasting: A literature review,” Expert Systems with Applications, vol. 197, p. 116659, Jul. 2022, doi: https://doi.org/10.1016/j.eswa.2022.116659\nQiao, R., Chen, W., & Qiao, Y. (2022). “Prediction of stock return by LSTM neural network,” Applied Artificial Intelligence, 36(1). doi: https://doi.org/10.1080/08839514.2022.2151159.\nFjellström, C. (2022, January 20). “Long short-term memory neural network for Financial Time Series,” arXiv.org. doi: https://doi.org/10.48550/arXiv.2201.08218.\nMehlig, B. (2021, October 27). “Machine learning with neural networks,” arXiv.org. doi: https://doi.org/10.48550/arXiv.1901.05639.\nStaudemeyer, R., Morris, E. (2019, September 23). “Understanding LSTM – a tutorial into Long Short-Term Memory Recurrent Neural Networks,” ArXiv.org. doi: https://doi.org/10.48550/arXiv.1909.09586\nSharma, S., Athaiya, A. (2020, April). “ACTIVATION FUNCTIONS IN NEURAL NETWORKS,” International Journal of Engineering Applied Sciences and Technology. 04. 310-316. 10.33564/IJEAST.2020.v04i12.054.\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585, 357–362. https://doi.org/10.1038/s41586-020-2649-2\nMcKinney, W., & others. (2010). Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference (Vol. 445, pp. 51–56).\nAbadi, Mart’in, Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … others. (2016). Tensorflow: A system for large-scale machine learning. In 12th $USENIX$ Symposium on Operating Systems Design and Implementation ($OSDI$ 16) (pp. 265–283).\nPedregosa, F., Varoquaux, Ga”el, Gramfort, A., Michel, V., Thirion, B., Grisel, O., … others. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12(Oct), 2825–2830.\nChollet, F., & others. (2015). Keras. GitHub. Retrieved from https://github.com/fchollet/keras.\nBluesky Capital and A. Leccese, \"Machine Learning in Finance: Why You Should Not Use LSTM's to Predict the Stock Market,\" Bluesky Capital, Jan. 13, 2019. https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/\n‌"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LSTM Recurrent Neural Networks",
    "section": "",
    "text": "Preface\nOur analysis and report on Long Short-Term Memory (LSTM) recurrent neural networks."
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "2  Data",
    "section": "",
    "text": "This project will be focused on predicting the percent price change on a given day from historical financial data markets. For this analysis, we’ll be using historical time-series data for Apple, a corporation listed on the New York Stock Exchange. That company is Apple, and we chose Apple due to its high trading volume and relatively low volatility. At the end of our analysis, we’ll also compare the results to a second company also traded on the NYSE, General Motors.\nWe decided to pull data going back the last 5 years, although this was an arbitrary decision. The data is sourced from Yahoo Finance using the yfinance package in python. yfinance is an open-source package that uses Yahoo Finance’s publicly available APIs to download market data.\n2.1 Data Variables Description\nSeveral variables are retrieved for each stock. The definitions for the variables are as follows :\nDate : This represents the date associated with each data point, such as daily stock prices.\nOpen : The stock price at the time of opening on a given trading day\nClose : The stock price at the time of closing on a given trading day\nHigh : The highest stock price reached on a given day.\nLow : The lowest stock price reached on a given day.\nDividends : The amount of dividends paid to shareholders.\nSplit ratio : If there have been stock splits, this variable indicates the split ratio.\nAdj Close : The closing stock price adjusted for any corporate actions such as dividends and stock splits.\nVolume : The total number of shares traded on a given day.\nMarket Capitalization : The total market value of the market’s shares.\nIn addition to the variables to the above variables, we are also going to add two more variables.\nPCT Change - this calculates the percentage change of the closing price.\nMoving Average - this variable will represent the average change of closing price for the series.\n2.2 Data Visualizations"
  },
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "3  Methodology",
    "section": "",
    "text": "This is the methodology section\n3.1\n3.2\n3.3 Implementation\nOur LSTM model for this project will be implemented in Python using the scikit-learn machine learning library and the Keras package. Keras is a high-level, deep learning API developed by Google for implementing neural networks built on top of the TensowFlow platform. Scikit-learn will be used to prepare our numeric dataset for an LSTM network, and to split our data into an 80/20 training set and testing set. The data must be scaled to achieve the best results. Large inputs tend to slow down the learning convergence. We have two options for scaling our data, normalization, and standardization. Normalization takes our data and outputs it in range between [0-1]. Standardization re-scales our data so that the mean is 0 and the standard deviation equals 1. For this project we will implement normalization. We will accomplish this by importing MinMaxScaler from the sklearn.preprocessing library. We will then create a MinMaxScaler object defining a range between 0-1, and lastly run the fit_transform method on our data set. To split our data, we will create 4 variables, LSTM_Xtrain, LSTM_Xtest, LSTM_ytrain, and LSTM_ytest to hold our training/testing sets. This will be done using the train_test_split() method that is part of the Scikit-learn library."
  },
  {
    "objectID": "intro.html#what-is-lstm",
    "href": "intro.html#what-is-lstm",
    "title": "1  Introduction",
    "section": "1.1 What is LSTM?",
    "text": "1.1 What is LSTM?\nLong short-term memory networks are a type of recurrent neural network that is specifically designed to solve sequence prediction problems. Sequence prediction problems are problems involving predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. Historically, other RNNs (Recurrent Neural Network) have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either so small that it won’t affect the output (vanishing gradients) or too large that it results in an overflow (exploding gradients). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget gate decides what information to discard from the cell, the Input gate decides which values from the input to update the memory cell, and the output gate decides what to output based on input and the memory of the cell."
  },
  {
    "objectID": "intro.html#applications-of-lstm-networks",
    "href": "intro.html#applications-of-lstm-networks",
    "title": "1  Introduction",
    "section": "1.2 Applications of LSTM Networks",
    "text": "1.2 Applications of LSTM Networks\nLong short term memory (LSTM) models are used in a wide range of situations. The influence of the LSTM network has been notable in natural language modeling, speech recognition, machine translation, and other applications[1]. LSTM networks were mainly created to solve the exploding/vanishing gradient problem[1]. Their capacity to model and understand long range dependencies makes them critical in executing various tasks. The advantage of using LSTMs over other recurrent neural networks is that an LSTM is able to save the data for much longer periods[2] than their RNN counterparts. They are especially essential in the field of finance because they can solve several problems such as identifying complex patterns in past pricing data, forecasting stock prices and the movement of the financial markets as a whole. With their ability to handle complicated sequential data, LSTMs have revolutionized how we approach and resolve issues involving sequences of information. They have become an essential tool in many machine learning and artificial intelligence fields."
  },
  {
    "objectID": "Data.html#data-description",
    "href": "Data.html#data-description",
    "title": "2  Data",
    "section": "2.1 Data Description",
    "text": "2.1 Data Description\nThe resulting data file is relatively simple, comprised of only seven columns, with each row representing an individual trading day going back to five years. The NYSE is only open on weekdays, excluding most Federal holidays. Thus, our data set contains 1,258 trading days, the amount of days in the last five years the stock market was at least partially open."
  },
  {
    "objectID": "Data.html#standard-variables",
    "href": "Data.html#standard-variables",
    "title": "2  Data",
    "section": "2.1 Standard Variables",
    "text": "2.1 Standard Variables\nDate : This represents a specific trading day that the market was at least partially open.\nOpen : The price at the time of market opening on the given day.\nHigh : The highest price that the stock reached on the given day.\nLow : The lowest price that the stock reached on the given day.\nClose : The price at the time of market closing on the given day.\nAdj Close : The closing stock price adjusted for any corporate actions such as dividends and stock splits.\nVolume : The total number of shares traded on the given day.\n\n\n\n\n\nTable 1: Preview of the first 5 rows of the dataset"
  },
  {
    "objectID": "Data.html#additional-variables",
    "href": "Data.html#additional-variables",
    "title": "2  Data",
    "section": "2.2 Additional Variables",
    "text": "2.2 Additional Variables\nIn addition to the variables to the standard variables fetched from the Yahoo Finance API, we created four of our own columns to add to the dataset.\nCompany Name : This variable simply denotes the company name that the data set belongs to.\nPercent Change : This variable calculates what percent the price changed from the previous closing price.\nIt’s important because we are trying to predict what percent the price will shift.\nThe percent change variable was created using the pandas “pct_change” command, applying it to the closing price only going back one period, thus creating a variable showing what percent the price moved each trading day, compared to the previous trading day.\nAAPL['pct_change'] = AAPL.Close.pct_change(periods = 1)\nSimple Moving Average : A simple moving average (SMA) is created by taking the mean closing price of a stock over a given number of periods. We created the SMA going back 20 time periods, so that each value represents the average closing price of the last 20 trading days.\nThis variable was created by using the pandas “rolling” command going back 20 periods. Because the first 19 days of the dataset (the oldest ones) do not have 20 previous periods to look to, they will instead have NULL values. We’ll drop those 19 rows so that our dataset contains no null values.\nAAPL['SMA20'] = AAPL['Close'].rolling(20).mean()\nExponential Moving Average : Given that we are trying to predict the price change of a stock in the short-term, exponential weighted moving averages (EMA) provide a benefit that SMA’s do not. We can generally assume that tomorrow’s stock price for a given company is going to be influenced by today’s closing price as compared to the closing price 20 days ago, SMA’s contain an inherent weakness for our analysis. SMA’s weigh each price point the same, so that in a 20 series average, each price point is weighed at an even 5%. EMA’s give additional weight to the most recent days, so day 20 will have a higher weight than day 19, which will have a higher weight than day 18, and so on.\nThis variable was created by using the pandas “ewm” command going back 20 periods.\nAAPL['EMA20'] = AAPL['Close'].ewm(span=20).mean()"
  },
  {
    "objectID": "Data.html#data-visualizations",
    "href": "Data.html#data-visualizations",
    "title": "2  Data",
    "section": "2.3 Data Visualizations",
    "text": "2.3 Data Visualizations"
  },
  {
    "objectID": "Data.html#data-summary-visualizations",
    "href": "Data.html#data-summary-visualizations",
    "title": "2  Data",
    "section": "2.3 Data Summary & Visualizations",
    "text": "2.3 Data Summary & Visualizations\nWe know that outside of the date variable, which acts as the index for this dataset, there are 6 other data points that came from the API. We created four more variables and dropped null values. Our resulting dataset contains 1,238 trading days. We can tell from using the pandas “info” command that all the variables are floats, with the exception of volume, which is an integer, and company name, which is an object. We also verify that there are no null values in our data.\n\n\n\n\n\nFigure 1: Dataframe information of the dataset.\nBy running the “describe” function, we can get a better idea of our dataset.\n\n\n\n\n\nTable 2: Summary statistics of the dataset\nDuring our five-year period, Apple’s closing price has ranged from $34.16 to $196.19, with the mean closing price being $117.78 and the median closing price at $130.86.\nWhen we plot the closing price by date, we can see that the price has been moving steadily upwards throughout the five year period, with occasional decreases.\n\n\n\n\n\nFigure 2: Closing price by day for AAPL\nFrom the “describe” function pictured above, we can also tell that the percent change of the stock price has ranged as low as 12.8% lower from the previous trading days closing price, to as high as 11.9% higher. As we are trying to predict percent change, it was important to see a histogram of the datapoint.\n\n\n\n\n\nFigure 3: Histogram of the daily percent change for AAPL\nThe histogram shows that the data is pretty evenly distributed, with a slight right tail."
  },
  {
    "objectID": "intro.html#what-is-an-lstm-network",
    "href": "intro.html#what-is-an-lstm-network",
    "title": "1  Introduction",
    "section": "1.1 What is an LSTM Network?",
    "text": "1.1 What is an LSTM Network?\nLSTM networks are a type of recurrent neural network that works well with sequential data, such as time series data and text. LSTMs are designed to solve sequence prediction problems. Such sequence prediction problems involve predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. In theory, RNN’s would be able to solve such a problem. Historically, however, and when dealing with real-world datasets, RNN’s have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either too small that it won’t affect the output (vanishing gradients problem) or too large that it results in an overflow (exploding gradients problem). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget Gate decides what information to discard from the cell, the Input Gate decides which values from the input to update the memory cell, and the Output Gate decides what to output based on input and the memory of the cell."
  },
  {
    "objectID": "intro.html#previous-research",
    "href": "intro.html#previous-research",
    "title": "1  Introduction",
    "section": "1.3 Previous Research",
    "text": "1.3 Previous Research\nTrying to predict stock market returns is a tale as old as time. There has been extensive research published on the ability to forecast financial markets through machine learning [3]. However, very few papers have focused specifically on LSTM models, which are ideal for such a time-series prediction. These papers have all been published in the last few years, making research on this topic novel and additional research vital to further understand the prediction value of LSTM neural networks as it applies to the financial markets.\nIn one 2022 paper conducted by researchers at a Chinese university, the authors applied an LSTM model to Chinese stock market data going back three years and found that the LSTM model performed better than linear regression model [4]. They concluded their research by stating that “stock returns are predictable to some extent” and that LSTM models “can help improve the prediction” potential of such returns.\nAnother 2022 paper, this one published Swedish researcher Dr. Carmina Fjellström, looked at the ability of LSTM models to select better-performing stock portfolios [5]. She took a Swedish stock index with the 29 most-traded companies in Sweden (OMX30) and downloaded the daily closing prices dating back 18 years. After running an LSTM model, she compared the results of the portfolio selected by the LSTM to the index as a whole (all 29 stocks), as well as to a randomly chosen portfolio. Dr. Fjellström found that LSTM portfolio had the highest average daily return of the three."
  },
  {
    "objectID": "methodology.html#neural-networks",
    "href": "methodology.html#neural-networks",
    "title": "3  Methodology",
    "section": "3.1 Neural Networks",
    "text": "3.1 Neural Networks\nGiven that LSTM models are an advanced form of RNN, one must first understand how RNN’s work and their limitations, so one can get an understanding of why LSTM models are needed. Before getting there, it’s worth a quick introduction into Neural Networks themselves.\nNeural Networks (also known as Artificial Neural Networks or ANN’s) are based on the same principle on how the human brain works: a network of neuron’s [6]. The network improves and learns by changing the connections between each neuron, which can also be called a node. In the case of machine learning, a neural network can take a supervised set of data with class labels and create nodes based on various input variables, and optimize the connections between each node using optimal strengths to be able to accurately predict unseen data. Thus, the network is made up entirely of individual nodes and individual connections between the nodes.\nA neural network must have at least one input node, at least one output node, and at least one hidden layer, which are layers of nodes between the Input and Output nodes. Often times, it will contain multiple inputs, hidden layers and outputs (Figure 4)\n\n\n\n\n\nFigure 4: An illustration of a neural network with two input nodes in the input layer, two hidden layers with four nodes each, and two output nodes, with a spider-web of connections between them\nThere are various learning techniques that one can set up for a neural network, called learning rules. The most common learning rule is backpropagation [7]. It uses gradient descent to learn the weights of each node, which are essentially the connections between the nodes. The goal of gradient descent is to take small, incremental steps meant at minimizing a loss function, such as the Mean-Squared Error (MSE), for each weight. The activation function (sometimes called the transfer function) takes those weights and defines an output based on the activation function formula. That output will then determine whether any specific neuron is activated. Neurons that are activated have their outputs fed to the next step of the training process.\nThere are various activation functions and their usefulness depends on various factors, including whether the data is linear or non-linear. There are three activation functions that are the most popular: Sigmoid, Tanh and ReLU. The Sigmoid function maps inputs, regardless of size, to an output ranging from 0 to 1. The Tanh function returns an output ranging from -1 to 1, while the ReLU function removes any negative part of the resulting outputs [8] (Figure 5).\n\n\n\n\n\nFigure 5: An illustration of the three most popular activation functions\n\n3.1.1 Recurrent Neural Networks\nMost ANN’s are feed-forward neural networks, so they are all fully connected and loop free. This means that each neuron in the network provides an input to a neuron in the following layer, never going back and sending an input to a neuron in a previous layer [8]. While this can be useful for some tasks, one of the most powerful benefits that ANN’s have is their ability to build a memory of time-series events by going back in time. This is done by having a circular connection between each layer, thus creating an Recurrent Neural Network. Unlike ANN’s, a RNN uses that circular connection to previous steps, therefore going back in time (Figure 6).\n\n\n\n\n\nFigure 6: An illustration of a recurrent neural network. Unlike an Artificial Neural Network, an RNN contains a feedback loop between the hidden layers\nRNN’s also need to be trained differently, as backpropgation on its own does not feed any information back to previous steps. In order to properly train an RNN, we need a feedback loop to do just that. There are two methods that are most common when training an RNN: Backpropagation through time (BPTT) and Real-time recurrent learning (RTRL), the only difference between the two being how they optimize the weights [8].\nDuring that training of an RNN, regardless of which method is used, we run into the issue of exploding and vanishing gradients. During either BPTT or RTRL, a given weight is multiplied by each input in the network. This essentially means the weight is multiplied to the power of how many inputs there are. Take an example of a stock market prediction neural network where 100 days of previous trading data act as the inputs. If a given weight is greater than 1, let’s say in this example the weight is 1.5, one would multiply 1.5 to the power of 100, resulting in an number over 400 quadrillion. Because the goal of the learning rule is to take small, incremental steps to find the optimal weights, this extremely large number will cause the gradient descent to take extremely large steps instead of finding that optimal weight, resulting in an exploding gradient [8].\nThis problem presents itself any time the weight is greater than 1. If you set the weight to below 1, you run into the opposite problem. Using the same stock market prediction, an example weight of 0.5 would then be multiplied to the 100th power, resulting in a number so small that it’s essentially zero. This causes the gradient descent to take steps that too small, never allowing it to find the best parameter values for the network, resulting in a vanishing gradient [8]."
  },
  {
    "objectID": "methodology.html#section-3.2",
    "href": "methodology.html#section-3.2",
    "title": "3  Methodology",
    "section": "3.2 Section 3.2",
    "text": "3.2 Section 3.2"
  },
  {
    "objectID": "methodology.html#implementation",
    "href": "methodology.html#implementation",
    "title": "3  Methodology",
    "section": "3.3 Implementation",
    "text": "3.3 Implementation\nOur LSTM model for this project will be implemented in Python using the scikit-learn machine learning library and the Keras package. Keras is a high-level, deep learning API developed by Google for implementing neural networks built on top of the TensowFlow platform. Scikit-learn will be used to prepare our numeric dataset for an LSTM network, and to split our data into an 80/20 training set and testing set. The data must be scaled to achieve the best results. Large inputs tend to slow down the learning convergence. We have two options for scaling our data, normalization, and standardization. Normalization takes our data and outputs it in range between [0-1]. Standardization re-scales our data so that the mean is 0 and the standard deviation equals 1. For this project we will implement normalization. We will accomplish this by importing MinMaxScaler from the sklearn.preprocessing library. We will then create a MinMaxScaler object defining a range between 0-1, and lastly run the fit_transform method on our data set. To split our data, we will create 4 variables, LSTM_Xtrain, LSTM_Xtest, LSTM_ytrain, and LSTM_ytest to hold our training/testing sets. This will be done using the train_test_split() method that is part of the Scikit-learn library."
  },
  {
    "objectID": "methodology.html#long-short-term-memory",
    "href": "methodology.html#long-short-term-memory",
    "title": "3  Methodology",
    "section": "3.2 Long Short-Term Memory",
    "text": "3.2 Long Short-Term Memory\nLSTMs are an improvement on the standard RNN models desgined to deal with more complex problems. Unlike a traditional RNN (which also involve a loop),they are able to handle long-term dependencies.. LSTMs were specifically designed to overcome the issues that come with training other RNNs such as vanishing and exploding gradient issues. In order to make the required prediction, they need to process not just individual data points but the entire sequence of data.They are composed of four components; the cell,input gate, the output gate and a forget gate [1].\n\n\n\n\n\nFigure 7: An illustration of an LSTM network\nThe gates are responsible for processing and regulating the information flowing in and out of the cell. The cell state carries the relevant information during the process of the sequence. Information is continually added to and removed from cell through the gates. The gates determine which information is important to keep or forget during the process.\nThe forget gate is responsible for discarding the information which is not gonna used in the next steps.\n\n\n\nForget gate equation\n\n\nIt determines which information from the previous cell state is retained[9]. The closer the value of the information passed through the forget is to 0, the more likely is to be forgotten.Therefore this means that the closer the value is to 1, the more likely it is to be kept. The input gate is in charge of choosing which new data points to include in the cell state at a certain time step [9].\n\n\n\nInput gate equation\n\n\nThe input gate regulates the information flow by producing values between 0 and 1 using a sigmoid activation function. The output gate decides which cell state data should be carried over to the following time step and added to the network’s output[9].\n\n\n\nOutput gate equation\n\n\nIt makes use of a sigmoid activation function and is essential to the LSTM’s capacity to provide outputs that are aware of context.\nThe sigmoid and tanh activation functions are crucial for the different gates and operations of the LSTM architecture in a typical LSTM network. Within the network, these activation functions are used to manage information flow and determine what should be updated, remembered, and forgotten.\n\n\n\n\n\nFigure 8: An illustration of activation functions used by LSTM networks\nThe sigmoid function is employed in the input, output and forget gates. In the input gate it is used to calculate the values from the current input and the previous hidden state to be between 0 and 1. In the forget gate, it calculates the input from the previous hidden state and current input . In the output gate, the sigmoid function also calculates the values of the current input and previous hidden state to produce the output . The tanh function is specifically employed in the output gate to determine which information from the cell state is going to be included in the hidden state and eventually in the network’s output ."
  },
  {
    "objectID": "Analysis.html",
    "href": "Analysis.html",
    "title": "4  Analysis",
    "section": "",
    "text": "#Downloading the Data\n\n#!pip install tensorflow\n#!pip install keras\n#!pip install matplotlib\n\nimport yfinance as yf\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib\n\n#Stocks that we'll be analyzing\nAAPL = ['AAPL']\nGM = ['GM']\n\n#Set start and end date for the data pull\n#We want to look at the past 5 years, so we'll pull the previous 5 years of data\nend_time = datetime.now()\nstart_time = datetime(end_time.year - 5, end_time.month, end_time.day)\n\n#download the stocks we want to model \nAAPL = yf.download(AAPL, start_time, end_time)\nGM = yf.download(GM, start_time, end_time)\n\n\n#Adding additional columns\nAAPL['pct_change'] = AAPL.Close.pct_change(periods = 1)\nAAPL['EMA20'] = AAPL['Close'].ewm(span=20).mean()\n\nGM['pct_change'] = GM.Close.pct_change(periods = 1)\nGM['EMA20'] = GM['Close'].ewm(span=20).mean()\n\n#AAPL.head(10)\n\n#Correlation Analysis \n#correlation = AAPL.corr()\n#print(correlation['Close'].sort_values(ascending=False))\n\n#Defining X and y variable\nX1 = AAPL[['Open','High', 'Low', 'Volume']]\nX1.head(10)\ny1 = AAPL['Close']\ny1.head(10)\n\nX2 = GM[['Open','High', 'Low', 'Volume']]\nX2.head(10)\ny2 = GM['Close']\ny2.head(10)\n\n#Converting to array\nX1 = X1.to_numpy()\ny1 = y1.to_numpy()\n\nX2 = X2.to_numpy()\ny2 = y2.to_numpy()\n\n#Splitting our data into 80/20 training/testing sets\nfrom sklearn.model_selection import train_test_split\nLSTM_Xtrain1, LSTM_Xtest1, LSTM_ytrain1, LSTM_ytest1 = train_test_split(X1, y1, test_size=0.2, random_state=1)\n\nLSTM_Xtrain2, LSTM_Xtest2, LSTM_ytrain2, LSTM_ytest2 = train_test_split(X2, y2, test_size=0.2, random_state=1)\n\nprint(LSTM_Xtrain1)\nprint(LSTM_Xtrain2)\n\n# Stacked LSTM model \n\nimport tensorflow as tf\nimport keras as ke\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\n#Apple model\nLSTM_modelApple = ke.Sequential()\nLSTM_modelApple.add(LSTM(128, return_sequences=True, input_shape=(LSTM_Xtrain1.shape[1], 1)))\nLSTM_modelApple.add(LSTM(64, return_sequences=False))\nLSTM_modelApple.add(Dense(25, activation='linear'))\nLSTM_modelApple.add(Dense(1))\n\n#GM model\nLSTM_modelGM = ke.Sequential()\nLSTM_modelGM.add(LSTM(128, return_sequences=True, input_shape=(LSTM_Xtrain2.shape[1], 1)))\nLSTM_modelGM.add(LSTM(64, return_sequences=False))\nLSTM_modelGM.add(Dense(25, activation='linear'))\nLSTM_modelGM.add(Dense(1))\n\nLSTM_modelApple.summary()\nLSTM_modelGM.summary()\n\nLSTM_modelApple.compile(optimizer='rmsprop' , loss= 'mean_squared_error')\n\nLSTM_modelGM.compile(optimizer='rmsprop' , loss= 'mean_squared_error')\n\nhistory1 = LSTM_modelApple.fit(LSTM_Xtrain1, LSTM_ytrain1, batch_size=32, epochs=10)\n\nhistory2 = LSTM_modelGM.fit(LSTM_Xtrain2, LSTM_ytrain2, batch_size=32,epochs=10)\n\n\nloss_test1= LSTM_modelApple.evaluate(LSTM_Xtest1, LSTM_ytest1)\nloss_test2 = LSTM_modelApple.evaluate(LSTM_Xtest2, LSTM_ytest2)\n\npredictions1 = LSTM_modelApple.predict(LSTM_Xtest1)\n\npredictions1.reshape(252,)\n\npredictions2 = LSTM_modelGM.predict(LSTM_Xtest2)\n\npredictions2.reshape(252,)\n\n#Predict 11/03/23 price = $177.57\nIn_featuresApple = np.array([[175.52, 177.78, 175.46, 76083900]])\nLSTM_modelApple.predict(In_featuresApple)\nprint(\"The predicted stock price for 11/03/23 is $ \", LSTM_modelApple.predict(In_featuresApple),\".\", \" The actual stock price for 11/03/23 is $177.57\")\n\n#Plot the first 50 predictions vs the actual y values\nimport matplotlib.pyplot as plt\nLSTM_ytest1 = LSTM_ytest1.reshape(252, 1)\n\nLSTM_ytest2 = LSTM_ytest2.reshape(252, 1)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n[[1.72259995e+02 1.73630005e+02 1.70820007e+02 4.95946000e+07]\n [1.61529999e+02 1.62470001e+02 1.61270004e+02 4.95017000e+07]\n [1.14570000e+02 1.15230003e+02 1.10000000e+02 1.80860300e+08]\n ...\n [1.57970001e+02 1.58490005e+02 1.55979996e+02 4.59922000e+07]\n [6.11274986e+01 6.12000008e+01 6.04524994e+01 6.92752000e+07]\n [1.52570007e+02 1.53100006e+02 1.50779999e+02 6.98583000e+07]]\n[[3.20499992e+01 3.21899986e+01 3.13099995e+01 1.30609000e+07]\n [3.62500000e+01 3.64799995e+01 3.58699989e+01 9.30020000e+06]\n [3.02999992e+01 3.05699997e+01 2.98700008e+01 1.36387000e+07]\n ...\n [3.44599991e+01 3.46500015e+01 3.40499992e+01 1.22142000e+07]\n [3.66199989e+01 3.67700005e+01 3.57400017e+01 6.71880000e+06]\n [4.09000015e+01 4.16100006e+01 4.07700005e+01 1.42857000e+07]]\n\n\nModel: \"sequential\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm (LSTM)                 (None, 4, 128)            66560     \n\n\n                                                                 \n\n\n lstm_1 (LSTM)               (None, 64)                49408     \n\n\n                                                                 \n\n\n dense (Dense)               (None, 25)                1625      \n\n\n                                                                 \n\n\n dense_1 (Dense)             (None, 1)                 26        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 117619 (459.45 KB)\n\n\nTrainable params: 117619 (459.45 KB)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\nModel: \"sequential_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_2 (LSTM)               (None, 4, 128)            66560     \n\n\n                                                                 \n\n\n lstm_3 (LSTM)               (None, 64)                49408     \n\n\n                                                                 \n\n\n dense_2 (Dense)             (None, 25)                1625      \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 26        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 117619 (459.45 KB)\n\n\nTrainable params: 117619 (459.45 KB)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\nEpoch 1/10\n\n\n 1/32 [..............................] - ETA: 5:14 - loss: 17850.5234\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 15644.5586  \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 15900.7969\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 15772.7217\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 14881.7344\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 14459.6035\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 14024.1689\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 13654.3691\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 13179.0322\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/32 [========================>.....] - ETA: 0s - loss: 13086.1475\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/32 [===========================>..] - ETA: 0s - loss: 12742.4707\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 11s 21ms/step - loss: 12606.0127\n\n\nEpoch 2/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 8693.1504\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 9772.4922\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 9170.6875\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 9002.3066\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 8641.8262\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 8623.2207\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 8586.4473\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 8393.2275\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 8248.5586\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 8114.9932\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 7880.6470\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 7868.6719\n\n\nEpoch 3/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 5713.8296\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 5922.1846\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 5842.7729\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 5897.3521\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 5945.6724\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 5823.5977\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 5688.9473\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 5461.6406\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 5356.8721\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/32 [========================>.....] - ETA: 0s - loss: 5271.1655\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/32 [===========================>..] - ETA: 0s - loss: 5127.4526\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 22ms/step - loss: 5041.5947\n\n\nEpoch 4/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 3034.0632\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 3556.1946\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 3487.9436\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 3493.4324\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 3464.1040\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 3481.0308\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 3361.7490\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 3292.1914\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 3271.4468\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 3220.5398\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 3095.1345\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 3089.4897\n\n\nEpoch 5/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 2102.0312\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 1699.1125\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 1639.6678\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 1590.3613\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 1472.7759\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 1409.2086\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 1361.2793\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 1301.4639\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 1238.5306\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 1204.3831\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 1164.0266\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 1155.6792\n\n\nEpoch 6/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 619.1258\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 828.0585\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 856.7490\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 716.8060\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 632.5975\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 573.7088\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 547.6782\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 518.9468\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 487.2103\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 465.4656\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 451.3146\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 448.6695\n\n\nEpoch 7/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 347.0085\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3/32 [=>............................] - ETA: 0s - loss: 296.7923\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 245.1817\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 219.7252\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 223.0440\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 249.5138\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/32 [==============>...............] - ETA: 0s - loss: 227.5485\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/32 [=================>............] - ETA: 0s - loss: 217.0573\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/32 [====================>.........] - ETA: 0s - loss: 211.6015\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 210.4924\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 203.7533\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 219.6363\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 219.6363\n\n\nEpoch 8/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 213.4327\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 133.0811\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 108.0463\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 118.1999\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 159.1105\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 139.9008\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 139.5156\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 135.0309\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 128.4955\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 141.6774\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 135.5284\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 134.5963\n\n\nEpoch 9/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 55.0438\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 107.0401\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 94.6184 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 155.8024\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 129.3708\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 112.3770\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 112.7955\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 131.7285\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 126.4455\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 119.4833\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 113.7246\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 113.8472\n\n\nEpoch 10/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 127.4089\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 76.9268 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 54.0622\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 45.3098\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 103.6735\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 98.6646 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 88.8192\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 80.0932\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 81.8757\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 88.9611\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 89.0983\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 88.6498\n\n\nEpoch 1/10\n\n\n 1/32 [..............................] - ETA: 4:48 - loss: 1773.3213\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 1537.0836  \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 1352.1608\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 1211.0248\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 1084.7893\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 957.5520 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 857.1149\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 774.2867\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 708.5995\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 652.3316\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 606.9402\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 10s 21ms/step - loss: 600.7240\n\n\nEpoch 2/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 80.7045\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 121.5190\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 145.9119\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 132.5465\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 119.7521\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 115.8692\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 113.3471\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 108.6190\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 108.2770\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 108.7407\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 109.9512\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 109.6814\n\n\nEpoch 3/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 68.2729\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 78.9483\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 83.9708\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 93.3768\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 92.0741\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 93.3890\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 90.5298\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 89.4009\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 85.3424\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 80.2891\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 78.1326\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 77.3099\n\n\nEpoch 4/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 45.8507\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 23.3488\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 23.6393\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 24.5916\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 21.8899\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 19.2271\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 19.8572\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 18.3936\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 16.5420\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 15.6097\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 15.4025\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 15.2536\n\n\nEpoch 5/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 4.4074\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 5.7328\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/32 [====>.........................] - ETA: 0s - loss: 5.1036\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/32 [=======>......................] - ETA: 0s - loss: 6.1751\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 10.4035\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 9.9158 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 8.3925\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 7.2127\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 6.7097\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 8.8793\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 8.5070\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 7.8676\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 22ms/step - loss: 7.8354\n\n\nEpoch 6/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 3.7876\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 4.6281\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 5.3822\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 8.0719\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 7.0538\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 6.0123\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 5.7606\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 7.2321\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 7.8451\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 7.1664\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 6.5498\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 6.4707\n\n\nEpoch 7/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 1.7170\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 1.7070\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 9.4125\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 8.4469\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 6.7784\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 5.7227\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 6.9527\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 8.6763\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 7.7715\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 7.0259\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 6.4263\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 6.3665\n\n\nEpoch 8/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 5.3137\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 9.4058\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 8.6938\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 6.4935\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 5.7911\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 6.1708\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 7.2777\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 7.3643\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 6.6420\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 6.1092\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 5.9298\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 5.9926\n\n\nEpoch 9/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 15.8345\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 9.6827 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 8.1831\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 6.1453\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 5.2620\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 5.0287\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 6.4515\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 5.9791\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 5.5776\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 5.2321\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 4.9010\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 5.6360\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 22ms/step - loss: 5.6360\n\n\nEpoch 10/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 10.8725\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 5.5644 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 3.6194\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 2.9519\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 4.3095\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 4.6350\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 4.1252\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 4.0169\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 4.1873\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 4.4110\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 4.3664\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 4.3573\n\n\n1/8 [==>...........................] - ETA: 14s - loss: 41.7166\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s - loss: 56.9148 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 2s 10ms/step - loss: 55.5127\n\n\n1/8 [==>...........................] - ETA: 0s - loss: 10.9799\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s - loss: 13.5169\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 10ms/step - loss: 13.5051\n\n\n1/8 [==>...........................] - ETA: 16s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 2s 9ms/step\n\n\n1/8 [==>...........................] - ETA: 13s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 2s 10ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 61ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 54ms/step\n\n\nThe predicted stock price for 11/03/23 is $  [[164.19055]] .  The actual stock price for 11/03/23 is $177.57\n\n\nApple Stock Price Prediction\n\nplt.plot(LSTM_ytest1[:50], 'red', label = 'AAPL Stock Price')\nplt.plot(predictions1[:50], color = 'green', label = 'Predicted AAPL Stock Price')\nplt.title('AAPL Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('AAPL Stock Price')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\nGM Stock Price Prediction\n\nplt.plot(LSTM_ytest2[:50], 'red', label = 'GM Stock Price')\nplt.plot(predictions2[:50], color = 'green', label = 'Predicted GM Stock Price')\nplt.title('GM Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('GM Stock Price')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\nTraining loss for Apple\n\nplt.close()\nplt.plot(history1.history['loss'], label='Loss')\nplt.title('Training Loss for Apple')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\nplt.close()\n\n\n\n\nTraining loss plot for GM\n\nplt.close()\nplt.plot(history2.history['loss'], label='Loss')\nplt.title('Training Loss for GM')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\nThe following table illustrates the fact that different numbers of epochs can yield different results when it comes to training. Training for an excessive number of epochs can cause overfitting, where the model begins to memorize the training data instead of generalizing well to new, unseen data, and underfitting, where the model fails to capture the underlying patterns in the data. Achieving the best training outcomes requires finding the correct balance.\n\n\n\nEpochs\nApple\nGM\n\n\n\n\n25\n\n\n\n\n50\n\n\n\n\n100\n\n\n\n\n150\n\n\n\n\n200"
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "Conclusion section"
  },
  {
    "objectID": "Data.html#correlations",
    "href": "Data.html#correlations",
    "title": "2  Data",
    "section": "2.4 Correlations",
    "text": "2.4 Correlations\nWe wanted to see how correlated the variables in our model are. We started by comparing the correlation between our target variable (Close) to the features of our Apple dataset.\nFrom the correlation table below, we see that the many of our feature variables are highly correlated to the target variable. Adjusted Close, Low, High, Open, and EMA20 are positively correlated with our target variable at a tune of 99% or higher. Volume is negatively correlated, which is an indication that on days that the trading volume of the stock is high, the the closing price decreases. PCT_CHANGE, the variable that contains how much the price changed from the previous date, has no correlation with the closing price.\n\n\n\n\n\nTable 3: Correlation between target variable and features"
  },
  {
    "objectID": "Analysis.html#pre-processing",
    "href": "Analysis.html#pre-processing",
    "title": "4  Analysis",
    "section": "4.1 Pre-Processing",
    "text": "4.1 Pre-Processing\nBased on the correlation results, we decided to model the Close price of the stock based on variables Open, High, Low, and Volume.\nFor pre-processing, we experimented with standardizing and normalizing our data, but we found that omitting both pre-processing steps resulted in a model with a lower Mean-Squared Error (MSE). In theory, a stock’s price has no upward limit and so bounding it on an interval of the min and max values does not make sense.\nOur next step is to split the data into an 80/20 split using train_test_split from the Scikit-learn library in Python. The 80% subset will be used for training our data and 20% will be used after compiling our model to test our model on unseen data. We decided on the 80/20 split because our original data set has 1,257 observations, which is enough data points to be able to train and test our model. Using the train_test_split method has the added benefit of also shuffling our data which prevents our model from prematurely overfitting to specific patterns in our data set."
  },
  {
    "objectID": "Analysis.html#model-building",
    "href": "Analysis.html#model-building",
    "title": "4  Analysis",
    "section": "4.2 Model Building",
    "text": "4.2 Model Building\nFor our model we first initialize a sequential class from the keras library [13] which will serve as the container for our neural network. Keras is built on top of the tensorflow library [11] in Python. We then add an LSTM layer with 128 neurons with the parameter ‘return_sequences’ set to TRUE which will return 128 cells at every time step and will then output 4 states for each step. Essentially this will return the LSTM current state in the hidden layer to be used as input for the next time step in the series.\nWe then stack another LSTM layer with 64 cells with the ‘return_sequences’ parameter set to FALSE which will only return the y output in the hidden layer to be used for input in the next time step. Lastly, we add a Dense output layer utilizing the “linear” activation function because our output is linear. Our output layer only utilizes one neuron because we are only interested in predicting the Close price of the stock.\n\n\n\n\n\nFigure 9: LSTM model output\nWe compile our model with the ‘rmsprop’ algorithm which uses backpropagation. We set the epoch to 150, which is the number of times the data will pass through the algorithm to find the optimal weights. Our loss variable is set to MSE, which will be the metric we use for evaluating our model. Next, we fit our model to our test X and test y variables. We get a final output of loss[MSE] of 8.6 at epoch 150 on our training subset.\n\n\n\n\n\nFigure 10: Result of the 150th epoch\nConsidering that the min and max features from the dataset is $37 and $197 respectively, the sum of the squared difference between our model predictions and actual y values equates to a model that does a good job at predicting the closing price. Furthermore, we can see from the output below (Figure 11) that at the 1st epoch our loss[MSE] is 14676.1, but after running 149 more times and updating the weights at each step, it was able to drastically bring down the MSE.\n\n\n\n\n\nFigure 11: Result of the 1st epoch"
  },
  {
    "objectID": "results.html#model-performance",
    "href": "results.html#model-performance",
    "title": "5  Results",
    "section": "5.1 Model Performance",
    "text": "5.1 Model Performance\nTo check our model’s performance on unseen data we pass through our testing subset ‘LSTM_Xtest’ which is a 2-dimensional array consisting of 252 rows by 4 columns and our testing y set which is a 1- dimensional array consisting of 252 rows by 1 column. We define a the loss variable the same as we did during the model building phase, where we selected MSE. After running the predictions on the test set, we get a loss[MSE] of 5.2.\n\n\n\n\n\nFigure 12: Final model result\nTo get a visual representation of the predicted values compared to the actual values, we define a predictions variable which uses the LSTM models predict function and pass that to the LSTM_Xtest array. We plot in a line graph the first 50 (Figure 13) data points and the last 50 data points (Figure 14) to get a sense of how the model predicted at both ends of the time series. We also chose to do this because it would be difficult to plot all 252 test observations on one graph. The red line on each graph represents the actual stock price and the green line represents the predicted stock price.\n\n\n\n\n\nFigure 13: Actual Price vs Predicted Price for Apple; first 50 days\n\n\n\n\n\nFigure 14: Actual Price vs Predicted Price for Apple; last 50 days\nAs can be seen in both graphs, the model does exceptionally well at predicting the closing price.\n\n5.1.1 Results from other days\nWe also wanted to evaluate how well our model would predict Apple’s stock price outside the 5 years of data we used in our original data set ending on 10/31/2023. To do this, we randomly selected 2 dates not in that original dataset: 11/03/23 and 11/07/2023. We created a 2 by 4 array with the Open, High, Low, and Volume values for each day and passed in the array to the models predict method.\n\n\n\n\n\nFigure 15: Array containing features for 11/03/23 and 11/07/23\n\n\n\n\n\nTable 4: Prediction results from the two arrays\nWe can see that the model was also able to accurately predict days that were not in our original dataset used to build the model.\n\n\n5.1.2 Results on other stocks\nAll things being the equal for our model architecture, we wanted to test the predictive performance of an LSTM Deep Neural network on another stock. For this purpose, we picked General Motors (GM). We picked this stock because it had different price trends and volatility than Apple, giving us a good comparison.The model summary is the same as compared to Apple because we are using the same five years of data and the same train/test split, except with the values representing the GM stock instead of Apple.\n\n\n\n\n\nFigure 16: LSTM model summary for General Motors\nAt the first epoch, we have a loss[MSE] of 1128.90 (Figure 17). By the 150th epoch the loss[MSE] drops to 0.78 (Figure 18) on our training data, an even better MSE than we saw on the Apple stock. When we apply it to the testing dataset, we get an MSE of 1.42, also lower than what we saw with Apple (Figure 19)\n\n\n\n\n\nFigure 17: First epoch for General Motors\n\n\n\n\n\nFigure 18: 150th epoch for General Motors\n\n\n\n\n\nFigure 19: LSTM results on unseen General Motors testing data\nSimilarly, when we look at the plot of predictions for both the first 50 days (Figure 20) and the last 50 days (Figure 21), we see that the model was able to accurately predict most days, just as we saw with Apple.\n\n\n\n\n\nFigure 20: Actual Price vs Predicted Price for General Motors; first 50 days\n\n\n\n\n\nFigure 21: Actual Price vs Predicted Price for General Motors; first 50 days"
  },
  {
    "objectID": "results.html#epoch-analysis",
    "href": "results.html#epoch-analysis",
    "title": "5  Results",
    "section": "5.2 Epoch Analysis",
    "text": "5.2 Epoch Analysis\nThe following illustration (Figure 22) show that different numbers of epochs can yield different results when it comes to training. Training for an excessive number of epochs can cause overfitting, where the model begins to memorize the training data instead of generalizing well to new, unseen data. Training too few epochs can result in underfitting, where the model fails to capture the underlying patterns in the data. Achieving the best training outcomes requires finding the correct balance. In the case of our analysis, we decided that 150 epochs yielded the best results for our LSTM model.\n\n\n\nEpochs\nApple\nGM\n\n\n\n\n25\n\n\n\n\n50\n\n\n\n\n100\n\n\n\n\n150\n\n\n\n\n200\n\n\n\n\n\nFigure 22: Apple and General Motors predictions plots based on number of epochs"
  },
  {
    "objectID": "Conclusion.html#other-considerations",
    "href": "Conclusion.html#other-considerations",
    "title": "6  Conclusion",
    "section": "6.1 Other Considerations",
    "text": "6.1 Other Considerations\nOur analysis shows that LSTM models can accurately predict the price of a stock based on its historical data. However, one should be cautious when using such a model in financial transactions without additional research. One reason is because previous research and analysis on stock market data shows that the most accurate prediction of tomorrow’s closing stock price is today’s closing stock price [14].\nOne way to show that relationship visually is by creating a plot with two lines, one representing the actual closing price of Apple, and another representing the one-day lagged closing price for Apple (Figure 23). For example, for the point representing Tuesday, October 3rd 2023, one point on the plot will show the closing price for Apple on that day, and the other point represents the closing price for Apple on October 2nd. For this plot, we are looking at the last year of the 5-year dataset, running from November 2022 through October 2023.\n\n\n\n\n\nFigure 23: Apple Closing Price vs 1-day lagged Closing Price\nWe can see that simply predicting that tomorrow’s price will be about the same as today’s price produces results just as good, if not better, than the LSTM model. That leads to the possible concern that the LSTM is doing precisely that, simply basing its predicting on the price of the previous day."
  },
  {
    "objectID": "Conclusion.html#summary",
    "href": "Conclusion.html#summary",
    "title": "6  Conclusion",
    "section": "6.2 Summary",
    "text": "6.2 Summary\nOur LSTM model was able to accurately predict the price of Apple’s stock using historical trading data. In our five-year dataset, Apple’s stock has had low volatility and has generally trended up throughout that time. We wanted to also test the model on a stock that hasn’t performed as well over the last five years, which is why we chose General Motors as a comparison. Whereas Apple’s stock has risen over 370% in that timeframe, GM has decreased over 15%, and has seen much more volatility in that time.\nOur analysis and results show that LSTM models do hold predictive power when it comes to financial markets, but that further research has to be done to ensure the model isn’t simply spitting out numbers that resemble the previous days closing price."
  },
  {
    "objectID": "appendix1.html#downloading-the-data",
    "href": "appendix1.html#downloading-the-data",
    "title": "7  Appendix",
    "section": "7.1 Downloading the Data",
    "text": "7.1 Downloading the Data\n\n#!pip install tensorflow\n#!pip install keras\n#!pip install matplotlib\n\nimport yfinance as yf\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib\n\n#Stocks that we'll be analyzing\nAAPL = ['AAPL']\nGM = ['GM']\n\n#Set start and end date for the data pull\n#We want to look at the past 5 years, so we'll pull the previous 5 years of data\nend_time = datetime.now()\nstart_time = datetime(end_time.year - 5, end_time.month, end_time.day)\n\n#download the stocks we want to model \nAAPL = yf.download(AAPL, start_time, end_time)\nGM = yf.download(GM, start_time, end_time)\n\n\n#Adding additional columns\nAAPL['pct_change'] = AAPL.Close.pct_change(periods = 1)\nAAPL['EMA20'] = AAPL['Close'].ewm(span=20).mean()\n\nGM['pct_change'] = GM.Close.pct_change(periods = 1)\nGM['EMA20'] = GM['Close'].ewm(span=20).mean()\n\n#AAPL.head(10)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n[*********************100%%**********************]  1 of 1 completed"
  },
  {
    "objectID": "appendix1.html#correlation-analysis",
    "href": "appendix1.html#correlation-analysis",
    "title": "7  Appendix",
    "section": "7.2 Correlation Analysis",
    "text": "7.2 Correlation Analysis\n\n#correlation = AAPL.corr()\n#print(correlation['Close'].sort_values(ascending=False))"
  },
  {
    "objectID": "appendix1.html#defining-x-and-y",
    "href": "appendix1.html#defining-x-and-y",
    "title": "7  Appendix",
    "section": "7.3 Defining X and y",
    "text": "7.3 Defining X and y\n\nX1 = AAPL[['Open','High', 'Low', 'Volume']]\nX1.head(10)\ny1 = AAPL['Close']\ny1.head(10)\n\nX2 = GM[['Open','High', 'Low', 'Volume']]\nX2.head(10)\ny2 = GM['Close']\ny2.head(10)\n\n#Converting to array\nX1 = X1.to_numpy()\ny1 = y1.to_numpy()\n\nX2 = X2.to_numpy()\ny2 = y2.to_numpy()"
  },
  {
    "objectID": "appendix1.html#traintest-split",
    "href": "appendix1.html#traintest-split",
    "title": "7  Appendix",
    "section": "7.4 Train/Test Split",
    "text": "7.4 Train/Test Split\n\nfrom sklearn.model_selection import train_test_split\nLSTM_Xtrain1, LSTM_Xtest1, LSTM_ytrain1, LSTM_ytest1 = train_test_split(X1, y1, test_size=0.2, random_state=1)\n\nLSTM_Xtrain2, LSTM_Xtest2, LSTM_ytrain2, LSTM_ytest2 = train_test_split(X2, y2, test_size=0.2, random_state=1)\n\nprint(LSTM_Xtrain1)\nprint(LSTM_Xtrain2)\n\n[[1.35899994e+02 1.36389999e+02 1.33770004e+02 6.42800000e+07]\n [1.72619995e+02 1.73039993e+02 1.69050003e+02 6.69218000e+07]\n [1.49119995e+02 1.49210007e+02 1.45550003e+02 1.04818600e+08]\n ...\n [1.59369995e+02 1.61050003e+02 1.59350006e+02 5.13057000e+07]\n [6.07900009e+01 6.16824989e+01 6.07200012e+01 7.34772000e+07]\n [1.50639999e+02 1.55229996e+02 1.50639999e+02 8.33226000e+07]]\n[[5.36100006e+01 5.37999992e+01 5.23499985e+01 2.98714000e+07]\n [3.24099998e+01 3.25499992e+01 3.19699993e+01 8.33910000e+06]\n [5.52099991e+01 5.53600006e+01 5.42200012e+01 1.33749000e+07]\n ...\n [3.48899994e+01 3.57299995e+01 3.47099991e+01 1.32900000e+07]\n [3.57799988e+01 3.68199997e+01 3.57599983e+01 8.73560000e+06]\n [4.12599983e+01 4.17799988e+01 4.07400017e+01 1.15425000e+07]]"
  },
  {
    "objectID": "appendix1.html#stacked-lstm-model",
    "href": "appendix1.html#stacked-lstm-model",
    "title": "7  Appendix",
    "section": "7.5 Stacked LSTM model",
    "text": "7.5 Stacked LSTM model\n\nimport tensorflow as tf\nimport keras as ke\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout"
  },
  {
    "objectID": "appendix1.html#aapl-and-gm-models",
    "href": "appendix1.html#aapl-and-gm-models",
    "title": "7  Appendix",
    "section": "7.6 AAPL and GM Models",
    "text": "7.6 AAPL and GM Models\n\nLSTM_modelApple = ke.Sequential()\nLSTM_modelApple.add(LSTM(128, return_sequences=True, input_shape=(LSTM_Xtrain1.shape[1], 1)))\nLSTM_modelApple.add(LSTM(64, return_sequences=False))\nLSTM_modelApple.add(Dense(25, activation='linear'))\nLSTM_modelApple.add(Dense(1))\n\nLSTM_modelGM = ke.Sequential()\nLSTM_modelGM.add(LSTM(128, return_sequences=True, input_shape=(LSTM_Xtrain2.shape[1], 1)))\nLSTM_modelGM.add(LSTM(64, return_sequences=False))\nLSTM_modelGM.add(Dense(25, activation='linear'))\nLSTM_modelGM.add(Dense(1))"
  },
  {
    "objectID": "appendix1.html#model-summary",
    "href": "appendix1.html#model-summary",
    "title": "7  Appendix",
    "section": "7.7 Model Summary",
    "text": "7.7 Model Summary\n\nLSTM_modelApple.summary()\nLSTM_modelGM.summary()\n\nModel: \"sequential\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm (LSTM)                 (None, 4, 128)            66560     \n\n\n                                                                 \n\n\n lstm_1 (LSTM)               (None, 64)                49408     \n\n\n                                                                 \n\n\n dense (Dense)               (None, 25)                1625      \n\n\n                                                                 \n\n\n dense_1 (Dense)             (None, 1)                 26        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 117619 (459.45 KB)\n\n\nTrainable params: 117619 (459.45 KB)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________\n\n\nModel: \"sequential_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n lstm_2 (LSTM)               (None, 4, 128)            66560     \n\n\n                                                                 \n\n\n lstm_3 (LSTM)               (None, 64)                49408     \n\n\n                                                                 \n\n\n dense_2 (Dense)             (None, 25)                1625      \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 26        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 117619 (459.45 KB)\n\n\nTrainable params: 117619 (459.45 KB)\n\n\nNon-trainable params: 0 (0.00 Byte)\n\n\n_________________________________________________________________"
  },
  {
    "objectID": "appendix1.html#loss-function",
    "href": "appendix1.html#loss-function",
    "title": "7  Appendix",
    "section": "7.8 Loss Function",
    "text": "7.8 Loss Function\n\nLSTM_modelApple.compile(optimizer='rmsprop' , loss= 'mean_squared_error')\n\nLSTM_modelGM.compile(optimizer='rmsprop' , loss= 'mean_squared_error')"
  },
  {
    "objectID": "appendix1.html#model-fit",
    "href": "appendix1.html#model-fit",
    "title": "7  Appendix",
    "section": "7.9 Model Fit",
    "text": "7.9 Model Fit\n\nhistory1 = LSTM_modelApple.fit(LSTM_Xtrain1, LSTM_ytrain1, batch_size=32, epochs=10)\nhistory2 = LSTM_modelGM.fit(LSTM_Xtrain2, LSTM_ytrain2, batch_size=32,epochs=10)\n\nloss_test1= LSTM_modelApple.evaluate(LSTM_Xtest1, LSTM_ytest1)\nloss_test2 = LSTM_modelApple.evaluate(LSTM_Xtest2, LSTM_ytest2)\n\nEpoch 1/10\n\n\n 1/32 [..............................] - ETA: 4:16 - loss: 17372.1543\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 16331.5283  \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 16374.1855\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/32 [==========>...................] - ETA: 0s - loss: 15558.9639\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/32 [=============>................] - ETA: 0s - loss: 14998.8096\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 14541.5020\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 14132.0479\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 13811.5654\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 13601.0732\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 13349.0996\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 9s 17ms/step - loss: 13349.0996\n\n\nEpoch 2/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 12088.0156\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 10131.2559\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 10400.6426\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 10416.9395\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 10402.9678\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/32 [==============>...............] - ETA: 0s - loss: 9906.1523 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/32 [==================>...........] - ETA: 0s - loss: 9835.1729\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 9633.7900\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 9416.2588\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 9185.9824\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 9050.1436\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 19ms/step - loss: 8971.8701\n\n\nEpoch 3/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 7065.3047\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 6784.5234\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/32 [====>.........................] - ETA: 0s - loss: 6476.1255\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/32 [=======>......................] - ETA: 0s - loss: 6575.6855\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/32 [==========>...................] - ETA: 0s - loss: 6704.9219\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/32 [=============>................] - ETA: 0s - loss: 6598.4976\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 6337.3784\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 6312.4341\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 6150.1538\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 6081.9536\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 19ms/step - loss: 5983.0947\n\n\nEpoch 4/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 5009.6162\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 4661.3149\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 4592.5669\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/32 [==========>...................] - ETA: 0s - loss: 4622.8892\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/32 [=============>................] - ETA: 0s - loss: 4430.5322\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 4212.4517\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 4162.2598\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 4049.9617\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 3851.7612\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 3779.9963\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 16ms/step - loss: 3779.9963\n\n\nEpoch 5/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 3012.2642\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 2920.1504\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/32 [=======>......................] - ETA: 0s - loss: 2671.4116\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/32 [==========>...................] - ETA: 0s - loss: 2367.4412\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 2237.6772\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 2103.3667\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/32 [====================>.........] - ETA: 0s - loss: 2006.5226\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/32 [========================>.....] - ETA: 0s - loss: 1896.5040\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/32 [===========================>..] - ETA: 0s - loss: 1839.5865\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 17ms/step - loss: 1814.1063\n\n\nEpoch 6/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 1076.3989\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 808.9908 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 786.2999\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/32 [==========>...................] - ETA: 0s - loss: 800.5769\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/32 [=============>................] - ETA: 0s - loss: 800.2237\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 729.4921\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 687.8390\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 646.7858\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 626.9469\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 17ms/step - loss: 612.1979\n\n\nEpoch 7/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 355.1623\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 393.8134\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/32 [=======>......................] - ETA: 0s - loss: 296.6349\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 308.9485\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 288.6058\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 300.1110\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 286.8965\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 273.4036\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 267.8954\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 286.3119\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 278.6085\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 21ms/step - loss: 276.3178\n\n\nEpoch 8/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 120.0625\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 125.1759\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/32 [====>.........................] - ETA: 0s - loss: 131.6890\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 172.1349\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 163.6282\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/32 [=============>................] - ETA: 0s - loss: 153.0613\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 146.2348\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 154.1753\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 156.5999\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 151.3885\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 146.3327\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 146.3327\n\n\nEpoch 9/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 119.3513\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 172.3398\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 138.1938\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 147.8038\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 130.9393\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 129.3018\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/32 [==================>...........] - ETA: 0s - loss: 125.3542\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 109.4559\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 111.7750\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 114.4378\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 18ms/step - loss: 114.4378\n\n\nEpoch 10/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 46.0500\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 84.1470\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/32 [=======>......................] - ETA: 0s - loss: 81.1383\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/32 [==========>...................] - ETA: 0s - loss: 69.7995\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 69.6852\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 62.7673\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/32 [====================>.........] - ETA: 0s - loss: 56.2036\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/32 [========================>.....] - ETA: 0s - loss: 78.7453\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/32 [===========================>..] - ETA: 0s - loss: 75.4544\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 17ms/step - loss: 77.5673\n\n\nEpoch 1/10\n\n\n 1/32 [..............................] - ETA: 4:21 - loss: 1837.0034\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 1538.7046  \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 1323.1244\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 1185.5620\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 1000.8923\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/32 [==============>...............] - ETA: 0s - loss: 914.9299 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/32 [=================>............] - ETA: 0s - loss: 834.9774\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/32 [====================>.........] - ETA: 0s - loss: 763.2570\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 708.8199\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 658.5297\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 9s 18ms/step - loss: 625.3185\n\n\nEpoch 2/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 122.2365\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 150.1059\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 149.6483\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/32 [========>.....................] - ETA: 0s - loss: 136.2927\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/32 [===========>..................] - ETA: 0s - loss: 129.1012\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 127.4902\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 125.5300\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/32 [====================>.........] - ETA: 0s - loss: 123.8954\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 119.0399\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 117.3208\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 18ms/step - loss: 116.4834\n\n\nEpoch 3/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 76.3379\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 113.3945\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 104.7033\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/32 [==========>...................] - ETA: 0s - loss: 99.5192 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/32 [=============>................] - ETA: 0s - loss: 99.4331\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 99.8669\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 97.8378\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/32 [==================>...........] - ETA: 0s - loss: 97.5575\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/32 [====================>.........] - ETA: 0s - loss: 95.3459\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 93.4168\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 90.5579\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 87.4903\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/32 [===========================>..] - ETA: 0s - loss: 83.8176\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 27ms/step - loss: 82.0155\n\n\nEpoch 4/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 48.6507\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 38.1192\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 27.5723\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 34.1333\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 29.5873\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 24.4884\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 24.0827\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 22.2559\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 19.7648\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 18.7865\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 18ms/step - loss: 18.7865\n\n\nEpoch 5/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 3.7948\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 21.8120\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 18.0318\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 14.2899\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 11.3856\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/32 [==============>...............] - ETA: 0s - loss: 10.3315\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 9.0750 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/32 [==================>...........] - ETA: 0s - loss: 8.5643\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 11.3445\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 10.5231\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 9.6392 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 20ms/step - loss: 9.5347\n\n\nEpoch 6/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 2.9909\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/32 [===>..........................] - ETA: 0s - loss: 9.0648\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 7.1460\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 6.0822\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 5.8618\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/32 [==============>...............] - ETA: 0s - loss: 7.6267\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/32 [==================>...........] - ETA: 0s - loss: 7.1269\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 6.9196\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 6.8491\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 7.0058\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 18ms/step - loss: 7.0184\n\n\nEpoch 7/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 0.7666\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 0.9764\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 1.0525\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 5.3332\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 7.0660\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 5.8636\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/32 [==================>...........] - ETA: 0s - loss: 5.1971\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 5.3237\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/32 [========================>.....] - ETA: 0s - loss: 6.6567\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 7.1097\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 17ms/step - loss: 7.0276\n\n\nEpoch 8/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 1.3230\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 0.9858\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 1.0421\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 1.7510\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/32 [=============>................] - ETA: 0s - loss: 6.0605\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 5.2633\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 4.4619\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/32 [======================>.......] - ETA: 0s - loss: 4.0003\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 3.5631\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 17ms/step - loss: 5.0499\n\n\nEpoch 9/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 21.8037\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 6.2209 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/32 [======>.......................] - ETA: 0s - loss: 3.3317\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/32 [==========>...................] - ETA: 0s - loss: 2.5420\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/32 [=============>................] - ETA: 0s - loss: 3.1021\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/32 [================>.............] - ETA: 0s - loss: 4.5324\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/32 [===================>..........] - ETA: 0s - loss: 4.0848\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/32 [=======================>......] - ETA: 0s - loss: 3.8783\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/32 [==========================>...] - ETA: 0s - loss: 4.2641\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - ETA: 0s - loss: 4.4300\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 17ms/step - loss: 4.4300\n\n\nEpoch 10/10\n\n\n 1/32 [..............................] - ETA: 0s - loss: 2.6413\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4/32 [==>...........................] - ETA: 0s - loss: 2.7335\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/32 [=====>........................] - ETA: 0s - loss: 4.8462\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/32 [=========>....................] - ETA: 0s - loss: 4.9473\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/32 [============>.................] - ETA: 0s - loss: 4.0697\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/32 [===============>..............] - ETA: 0s - loss: 3.4611\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/32 [==================>...........] - ETA: 0s - loss: 5.2105\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/32 [=====================>........] - ETA: 0s - loss: 5.5671\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/32 [=========================>....] - ETA: 0s - loss: 4.9047\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/32 [============================>.] - ETA: 0s - loss: 4.5141\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 1s 17ms/step - loss: 4.4796\n\n\n1/8 [==>...........................] - ETA: 19s - loss: 142.3585\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - ETA: 0s - loss: 138.4048 \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 3s 10ms/step - loss: 138.4048\n\n\n1/8 [==>...........................] - ETA: 0s - loss: 24.5946\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s - loss: 22.9925\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 0s 8ms/step - loss: 22.5986"
  },
  {
    "objectID": "appendix1.html#predictions",
    "href": "appendix1.html#predictions",
    "title": "7  Appendix",
    "section": "7.10 Predictions",
    "text": "7.10 Predictions\n\npredictions1 = LSTM_modelApple.predict(LSTM_Xtest1)\npredictions1.reshape(252,)\n\npredictions2 = LSTM_modelGM.predict(LSTM_Xtest2)\npredictions2.reshape(252,)\n\nIn_featuresApple = np.array([[175.52, 177.78, 175.46, 76083900]])\nLSTM_modelApple.predict(In_featuresApple)\nprint(\"The predicted stock price for 11/03/23 is $ \", LSTM_modelApple.predict(In_featuresApple),\".\", \" The actual stock price for 11/03/23 is $177.57\")\n\n1/8 [==>...........................] - ETA: 14s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 2s 9ms/step\n\n\n1/8 [==>...........................] - ETA: 11s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/8 [=========================>....] - ETA: 0s \n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/8 [==============================] - 2s 8ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 50ms/step\n\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 47ms/step\n\n\nThe predicted stock price for 11/03/23 is $  [[174.45453]] .  The actual stock price for 11/03/23 is $177.57"
  },
  {
    "objectID": "appendix1.html#first-50-days-plot",
    "href": "appendix1.html#first-50-days-plot",
    "title": "7  Appendix",
    "section": "7.11 First 50 Days Plot",
    "text": "7.11 First 50 Days Plot\n\nimport matplotlib.pyplot as plt\nLSTM_ytest1 = LSTM_ytest1.reshape(252, 1)\nLSTM_ytest2 = LSTM_ytest2.reshape(252, 1)\n\n\nplt.plot(LSTM_ytest1[:50], 'red', label = 'AAPL Stock Price')\nplt.plot(predictions1[:50], color = 'green', label = 'Predicted AAPL Stock Price')\nplt.title('AAPL Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('AAPL Stock Price')\nplt.legend()\nplt.show()\nplt.close()\n\n\nplt.plot(LSTM_ytest2[:50], 'red', label = 'GM Stock Price')\nplt.plot(predictions2[:50], color = 'green', label = 'Predicted GM Stock Price')\nplt.title('GM Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('GM Stock Price')\nplt.legend()\nplt.show()\nplt.close()"
  },
  {
    "objectID": "appendix1.html#training-loss",
    "href": "appendix1.html#training-loss",
    "title": "7  Appendix",
    "section": "7.12 Training Loss",
    "text": "7.12 Training Loss\n\nplt.close()\nplt.plot(history1.history['loss'], label='Loss')\nplt.title('Training Loss for Apple')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\nplt.close()\n\nplt.close()\nplt.plot(history2.history['loss'], label='Loss')\nplt.title('Training Loss for GM')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()"
  }
]
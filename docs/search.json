[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Long Short-Term Memory (LSTM) neural networks have become extremely popular in the deep learning and data science space in recent years. After being largely ignored for decades, this type of recurrent neural networks (RNN) have garnered significant attention due to their ability to model sequential data and capture long-term dependencies in data, making them perfectly suited for tasks such as language and speech processing, time series analysis, and much more. LSTM models get their power from their capacity to effectively handle the vanishing and exploding gradient problem seen with other RNN’s, enabling the modeling of sequential data with a great level of precision. As a result, LSTM networks have become a cornerstone of machine learning.\nThis paper dives into the history of LSTM neural networks, diving into previous literature and research studies conducted using LSTM. We then plan on putting an LSTM model to the test by seeing how accurately it can predict financial time series data, specifically looking at numerous companies listed on the New York Stock Exchange. While modeling financial data is an extremely difficult task due to the unpredictability of financial markets, we will explore whether they hold any potential to accurately predict short and medium term movements in stock prices. Our hope is this paper can spur further research into the ability to accurately model financial data and how such modeling can be improved in the years to come.\n1.1 What is LSTM\nLong short-term memory networks are a type of recurrent neural network that is specifically designed to solve sequence prediction problems. Sequence prediction problems are problems involving predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. Historically, other RNNs (Recurrent Neural Network) have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either so small that it won’t affect the output (vanishing gradients) or too large that it results in an overflow (exploding gradients). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget gate decides what information to discard from the cell, the Input gate decides which values from the input to update the memory cell, and the output gate decides what to output based on input and the memory of the cell.\n1.2 Applications of LSTM Networks\nLong short term memory (LSTM) models are used in a wide range of situations. The influence of the LSTM network has been notable in natural language modeling, speech recognition, machine translation, and other applications[1]. LSTM networks were mainly created to solve the exploding/vanishing gradient problem[1]. Their capacity to model and understand long range dependencies makes them critical in executing various tasks. The advantage of using LSTMs over other recurrent neural networks is that an LSTM is able to save the data for much longer periods[2] than their RNN counterparts. They are especially essential in the field of finance because they can solve several problems such as identifying complex patterns in past pricing data, forecasting stock prices and the movement of the financial markets as a whole. With their ability to handle complicated sequential data, LSTMs have revolutionized how we approach and resolve issues involving sequences of information. They have become an essential tool in many machine learning and artificial intelligence fields. \nReferences\n\nA. Sherstinsky, “Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network,” Physica D: Nonlinear Phenomena, vol. 404, p. 132306, Mar. 2020, doi:https://doi.org/10.1016/j.physd.2019.132306.\n\n\nP. Chaajer, M. Shah, and A. Kshirsagar, “The applications of artificial neural networks, support vector machines, and long-short term memory for stock market prediction,” Decision Analytics Journal, p. 100015, Nov. 2021, doi:\nhttps://doi.org/10.1016/j.dajour.2021.100015."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "A. Sherstinsky, “Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network,” Physica D: Nonlinear Phenomena, vol. 404, p. 132306, Mar. 2020, doi:https://doi.org/10.1016/j.physd.2019.132306.\nP. Chaajer, M. Shah, and A. Kshirsagar, “The applications of artificial neural networks, support vector machines, and long-short term memory for stock market prediction,” Decision Analytics Journal, p. 100015, Nov. 2021, doi: https://doi.org/10.1016/j.dajour.2021.100015.\nM. Kumbure, C. Lohrmann, P. Luukka, and J. Porras, “Machine learning techniques and data for stock market forecasting: A literature review,” Expert Systems with Applications, vol. 197, p. 116659, Jul. 2022, doi: https://doi.org/10.1016/j.eswa.2022.116659\nQiao, R., Chen, W., & Qiao, Y. (2022). Prediction of stock return by LSTM neural network. Applied Artificial Intelligence, 36(1). doi: https://doi.org/10.1080/08839514.2022.2151159.\nFjellström, C. (2022, January 20). Long short-term memory neural network for Financial Time Series. arXiv.org. doi: https://doi.org/10.48550/arXiv.2201.08218."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LSTM Recurrent Neural Networks",
    "section": "",
    "text": "Preface\nOur analysis and report on Long Short-Term Memory (LSTM) recurrent neural networks."
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "2  Data",
    "section": "",
    "text": "This project will be focused on predicting the percent price change on a given day from historical financial data markets. For this analysis, we’ll be using historical time-series data for Apple, a corporation listed on the New York Stock Exchange. That company is Apple, and we chose Apple due to its high trading volume and relatively low volatility. At the end of our analysis, we’ll also compare the results to a second company also traded on the NYSE, General Motors.\nWe decided to pull data going back the last 5 years, although this was an arbitrary decision. The data is sourced from Yahoo Finance using the yfinance package in python. yfinance is an open-source package that uses Yahoo Finance’s publicly available APIs to download market data.\n2.1 Data Variables Description\nSeveral variables are retrieved for each stock. The definitions for the variables are as follows :\nDate : This represents the date associated with each data point, such as daily stock prices.\nOpen : The stock price at the time of opening on a given trading day\nClose : The stock price at the time of closing on a given trading day\nHigh : The highest stock price reached on a given day.\nLow : The lowest stock price reached on a given day.\nDividends : The amount of dividends paid to shareholders.\nSplit ratio : If there have been stock splits, this variable indicates the split ratio.\nAdj Close : The closing stock price adjusted for any corporate actions such as dividends and stock splits.\nVolume : The total number of shares traded on a given day.\nMarket Capitalization : The total market value of the market’s shares.\nIn addition to the variables to the above variables, we are also going to add two more variables.\nPCT Change - this calculates the percentage change of the closing price.\nMoving Average - this variable will represent the average change of closing price for the series.\n2.2 Data Visualizations"
  },
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "3  Methodology",
    "section": "",
    "text": "This is the methodology section\n3.1\n3.2\n3.3 Implementation\nOur LSTM model for this project will be implemented in Python using the scikit-learn machine learning library and the Keras package. Keras is a high-level, deep learning API developed by Google for implementing neural networks built on top of the TensowFlow platform. Scikit-learn will be used to prepare our numeric dataset for an LSTM network, and to split our data into an 80/20 training set and testing set. The data must be scaled to achieve the best results. Large inputs tend to slow down the learning convergence. We have two options for scaling our data, normalization, and standardization. Normalization takes our data and outputs it in range between [0-1]. Standardization re-scales our data so that the mean is 0 and the standard deviation equals 1. For this project we will implement normalization. We will accomplish this by importing MinMaxScaler from the sklearn.preprocessing library. We will then create a MinMaxScaler object defining a range between 0-1, and lastly run the fit_transform method on our data set. To split our data, we will create 4 variables, LSTM_Xtrain, LSTM_Xtest, LSTM_ytrain, and LSTM_ytest to hold our training/testing sets. This will be done using the train_test_split() method that is part of the Scikit-learn library."
  },
  {
    "objectID": "intro.html#what-is-lstm",
    "href": "intro.html#what-is-lstm",
    "title": "1  Introduction",
    "section": "1.1 What is LSTM?",
    "text": "1.1 What is LSTM?\nLong short-term memory networks are a type of recurrent neural network that is specifically designed to solve sequence prediction problems. Sequence prediction problems are problems involving predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. Historically, other RNNs (Recurrent Neural Network) have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either so small that it won’t affect the output (vanishing gradients) or too large that it results in an overflow (exploding gradients). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget gate decides what information to discard from the cell, the Input gate decides which values from the input to update the memory cell, and the output gate decides what to output based on input and the memory of the cell."
  },
  {
    "objectID": "intro.html#applications-of-lstm-networks",
    "href": "intro.html#applications-of-lstm-networks",
    "title": "1  Introduction",
    "section": "1.2 Applications of LSTM Networks",
    "text": "1.2 Applications of LSTM Networks\nLong short term memory (LSTM) models are used in a wide range of situations. The influence of the LSTM network has been notable in natural language modeling, speech recognition, machine translation, and other applications[1]. LSTM networks were mainly created to solve the exploding/vanishing gradient problem[1]. Their capacity to model and understand long range dependencies makes them critical in executing various tasks. The advantage of using LSTMs over other recurrent neural networks is that an LSTM is able to save the data for much longer periods[2] than their RNN counterparts. They are especially essential in the field of finance because they can solve several problems such as identifying complex patterns in past pricing data, forecasting stock prices and the movement of the financial markets as a whole. With their ability to handle complicated sequential data, LSTMs have revolutionized how we approach and resolve issues involving sequences of information. They have become an essential tool in many machine learning and artificial intelligence fields."
  },
  {
    "objectID": "Data.html#data-description",
    "href": "Data.html#data-description",
    "title": "2  Data",
    "section": "2.1 Data Description",
    "text": "2.1 Data Description\nThe resulting data file is relatively simple, comprised of only seven columns, with each row representing an individual trading day going back to five years. The NYSE is only open on weekdays, excluding most Federal holidays. Thus, our data set contains 1,258 trading days, the amount of days in the last five years the stock market was at least partially open."
  },
  {
    "objectID": "Data.html#standard-variables",
    "href": "Data.html#standard-variables",
    "title": "2  Data",
    "section": "2.1 Standard Variables",
    "text": "2.1 Standard Variables\nDate : This represents a specific trading day that the market was at least partially open.\nOpen : The price at the time of market opening on the given day.\nHigh : The highest price that the stock reached on the given day.\nLow : The lowest price that the stock reached on the given day.\nClose : The price at the time of market closing on the given day.\nAdj Close : The closing stock price adjusted for any corporate actions such as dividends and stock splits.\nVolume : The total number of shares traded on the given day.\n\nTable 1: Preview of the first 5 rows of the dataset"
  },
  {
    "objectID": "Data.html#additional-variables",
    "href": "Data.html#additional-variables",
    "title": "2  Data",
    "section": "2.2 Additional Variables",
    "text": "2.2 Additional Variables\nIn addition to the variables to the standard variables fetched from the Yahoo Finance API, we created four of our own columns to add to the dataset.\nCompany Name : This variable simply denotes the company name that the data set belongs to.\nPercent Change : This variable calculates what percent the price changed from the previous closing price.\nIt’s important because we are trying to predict what percent the price will shift.\nThe percent change variable was created using the pandas “pct_change” command, applying it to the closing price only going back one period, thus creating a variable showing what percent the price moved each trading day, compared to the previous trading day.\nAAPL['pct_change'] = AAPL.Close.pct_change(periods = 1)\nSimple Moving Average : A simple moving average (SMA) is created by taking the mean closing price of a stock over a given number of periods. We created the SMA going back 20 time periods, so that each value represents the average closing price of the last 20 trading days.\nThis variable was created by using the pandas “rolling” command going back 20 periods. Because the first 19 days of the dataset (the oldest ones) do not have 20 previous periods to look to, they will instead have NULL values. We’ll drop those 19 rows so that our dataset contains no null values.\nAAPL['SMA20'] = AAPL['Close'].rolling(20).mean()\nExponential Moving Average : Given that we are trying to predict the price change of a stock in the short-term, exponential weighted moving averages (EMA) provide a benefit that SMA’s do not. We can generally assume that tomorrow’s stock price for a given company is going to be influenced by today’s closing price as compared to the closing price 20 days ago, SMA’s contain an inherent weakness for our analysis. SMA’s weigh each price point the same, so that in a 20 series average, each price point is weighed at an even 5%. EMA’s give additional weight to the most recent days, so day 20 will have a higher weight than day 19, which will have a higher weight than day 18, and so on.\nThis variable was created by using the pandas “ewm” command going back 20 periods.\nAAPL['EMA20'] = AAPL['Close'].ewm(span=20).mean()"
  },
  {
    "objectID": "Data.html#data-visualizations",
    "href": "Data.html#data-visualizations",
    "title": "2  Data",
    "section": "2.3 Data Visualizations",
    "text": "2.3 Data Visualizations"
  },
  {
    "objectID": "Data.html#data-summary-visualizations",
    "href": "Data.html#data-summary-visualizations",
    "title": "2  Data",
    "section": "2.3 Data Summary & Visualizations",
    "text": "2.3 Data Summary & Visualizations\nWe know that outside of the date variable, which acts as the index for this dataset, there are 6 other data points that came from the API. We created four more variables and dropped null values. Our resulting dataset contains 1,238 trading days. We can tell from using the pandas “info” command that all the variables are floats, with the exception of volume, which is an integer, and company name, which is an object. We also verify that there are no null values in our data.\n\nGraphic 1: Dataframe information of the dataset\nBy running the “describe” function, we can get a better idea of our dataset.\n\nTable 2: Summary statistics of the dataset\nDuring our five-year period, Apple’s closing price has ranged from $34.16 to $196.19, with the mean closing price being $117.78 and the median closing price at $130.86.\nWhen we plot the closing price by date, we can see that the price has been moving steadily upwards throughout the five year period, with occasional decreases.\n\nGraphic 2: Closing price by day for AAPL\nFrom the “describe” function pictured above, we can also tell that the percent change of the stock price has ranged as low as 12.8% lower from the previous trading days closing price, to as high as 11.9% higher. As we are trying to predict percent change, it was important to see a histogram of the datapoint.\n\nGraphic 3: Histogram of the daily percent change for AAPL\nThe histogram shows that the data is pretty evenly distributed, with a slight right tail."
  },
  {
    "objectID": "intro.html#what-is-an-lstm-network",
    "href": "intro.html#what-is-an-lstm-network",
    "title": "1  Introduction",
    "section": "1.1 What is an LSTM Network?",
    "text": "1.1 What is an LSTM Network?\nLSTM networks are a type of recurrent neural network that works well with sequential data, such as time series data and text. LSTMs are designed to solve sequence prediction problems. Such sequence prediction problems involve predicting the next value based on a given input. A simple example is the input [1, 2, 3, 4, 5] and the sequence prediction model would output [6] as the next digit in the sequence. In theory, RNN’s would be able to solve such a problem. Historically, however, and when dealing with real-world datasets, RNN’s have faced the challenge of the weights being changed too quickly after a few cycles, meaning the results would be either too small that it won’t affect the output (vanishing gradients problem) or too large that it results in an overflow (exploding gradients problem). LSTMs overcome this challenge by regulating the weights with three “gates.” The Forget Gate decides what information to discard from the cell, the Input Gate decides which values from the input to update the memory cell, and the Output Gate decides what to output based on input and the memory of the cell."
  },
  {
    "objectID": "intro.html#previous-research",
    "href": "intro.html#previous-research",
    "title": "1  Introduction",
    "section": "1.3 Previous Research",
    "text": "1.3 Previous Research\nTrying to predict stock market returns is a tale as old as time. There has been extensive research published on the ability to forecast financial markets through machine learning [3]. However, very few papers have focused specifically on LSTM models, which are ideal for such a time-series prediction. These papers have all been published in the last few years, making research on this topic novel and additional research vital to further understand the prediction value of LSTM neural networks as it applies to the financial markets.\nIn one 2022 paper conducted by researchers at a Chinese university, the authors applied an LSTM model to Chinese stock market data going back three years and found that the LSTM model performed better than linear regression model [4]. They concluded their research by stating that “stock returns are predictable to some extent” and that LSTM models “can help improve the prediction” potential of such returns.\nAnother 2022 paper, this one published Swedish researcher Dr. Carmina Fjellström, looked at the ability of LSTM models to select better-performing stock portfolios [5]. She took a Swedish stock index with the 29 most-traded companies in Sweden (OMX30) and downloaded the daily closing prices dating back 18 years. After running an LSTM model, she compared the results of the portfolio selected by the LSTM to the index as a whole (all 29 stocks), as well as to a randomly chosen portfolio. Dr. Fjellström found that LSTM portfolio had the highest average daily return of the three."
  }
]